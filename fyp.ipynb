{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda env update --file environment.yml --prune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, glob, logging, ntpath, math, time, sys\n",
    "from IPython import display\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# pd.options.display.max_seq_items = 2000\n",
    "pd.set_option(\"display.max_colwidth\",None)\n",
    "\n",
    "import os, random, glob, logging, ntpath, math, time, sys, datetime\n",
    "logging.basicConfig()\n",
    "logger=logging.getLogger(\"dbg\")\n",
    "logging.disable(logging.NOTSET)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "perf_logger=logging.getLogger(\"perf\")\n",
    "perf_logger.setLevel(logging.DEBUG)\n",
    "# logging.disable(logging.DEBUG)\n",
    "\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torchaudio.functional as audioF\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.debug(device)\n",
    "logger.debug(torch.__version__)\n",
    "logger.debug(torch.cuda.get_device_name(device))\n",
    "logger.debug(torchaudio.list_audio_backends())\n",
    "\n",
    "from ignite.engine import Engine, Events, EventEnum\n",
    "from ignite.metrics import Loss, Metric, RunningAverage\n",
    "from ignite.metrics.metric import reinit__is_reduced, sync_all_reduce\n",
    "from ignite.exceptions import NotComputableError\n",
    "from ignite.handlers.tqdm_logger import ProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "SHUFFLE = False\n",
    "FRAME_SHIFT = 40\n",
    "MAXIMUM_SAMPLE_NUM_OF_FRAMES = 640000\n",
    "\n",
    "RUN_GAN = False\n",
    "RUN_CRN = True\n",
    "RUN_RNN = False\n",
    "RUN_CNN = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystoi\n",
    "import pesq\n",
    "\n",
    "def combine_audio(speech: torch.Tensor, noise: torch.Tensor, snr: torch.Tensor | int) -> torch.Tensor:\n",
    "    if not (torch.is_floating_point(speech) or torch.is_complex(speech)):\n",
    "        # speech = torch.tensor(speech, dtype=torch.float64, device=speech.device)\n",
    "        speech = speech.to(torch.float64,non_blocking=True)\n",
    "    if not (torch.is_floating_point(noise) or torch.is_complex(noise)):\n",
    "        # noise = torch.tensor(noise, dtype=torch.float64, device=noise.device)\n",
    "        noise = noise.to(torch.float64,non_blocking=True)\n",
    "    if not(type(snr) is torch.Tensor):\n",
    "        snr = torch.tensor([snr])\n",
    "    logger.debug(f\"speech:{speech.ndim}, noise:{noise.ndim}, snr:{snr.ndim}\")\n",
    "    out = audioF.add_noise(speech, noise, snr).to(dtype=torch.float)\n",
    "    return out\n",
    "\n",
    "def calc_pesq(speech: np.ndarray, processed: np.ndarray) -> float:\n",
    "    return pesq.pesq(ref=speech, deg=processed, fs=16000)\n",
    "\n",
    "def calc_stoi(speech: np.ndarray, processed: np.ndarray) -> float:\n",
    "    return pystoi.stoi(x=speech, y=processed, fs_sig=16000)\n",
    "\n",
    "def ns_to_sec(ns: int):\n",
    "    return ns/1000000000.0\n",
    "\n",
    "def datetime_string():\n",
    "    return datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")\n",
    "\n",
    "def plot_waveform(waveform, sample_rate=16000):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "\n",
    "def write_fstring_file(model_name: str, format_string: str, **args):\n",
    "    with open(f\"saved_models/{model_name}_{datetime_string()}.txt\") as f:\n",
    "        f.write(format_string.format(**args))\n",
    "\n",
    "\n",
    "# def stitch_audio(frames_func, frame_size, frame_shift):\n",
    "#     *batches, at_end = frames_func()\n",
    "\n",
    "#     # audio[mix, clean]\n",
    "#     audio = [torch.zeros((b.shape[0],b.shape[1],640000),dtype=torch.float) for b in batches if len(b.shape)==3]\n",
    "#     pos=0\n",
    "#     end=frame_size\n",
    "#     for batch_i, batch in enumerate(batches):\n",
    "#         audio[batch_i][:,:,:end] = batch[:,:,:]\n",
    "\n",
    "#     frame_slice_start = frame_size - frame_shift\n",
    "#     while not at_end:\n",
    "#         pos = end\n",
    "#         end += frame_shift\n",
    "#         *batches, at_end = frames_func()\n",
    "#         for batch_i, batch in enumerate((batches)):\n",
    "#             audio[batch_i][:,0,pos:end] = batch[:,0,frame_slice_start:]\n",
    "\n",
    "#             if at_end:\n",
    "#                 print(audio[batch_i].shape)\n",
    "#                 audio[batch_i] = torch.tensor(audio[batch_i][:,:,:end]) \n",
    "        \n",
    "#     for i in range(BATCH_SIZE):\n",
    "#         print(audio[0][i][0])\n",
    "\n",
    "#     return audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ldrnd = random.Random(42)   #   Used for noise loading \n",
    "\n",
    "def get_sequential_wav_paths(dir):\n",
    "    count = len(glob.glob(\"*.wav\", root_dir=dir))\n",
    "    lst = []\n",
    "    for i in range(1,count+1):\n",
    "        lst.append(dir + \"/\" + str(i) + \".wav\")\n",
    "    \n",
    "    return lst\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data: list, root_dir: str | None = None):\n",
    "        self.data = data\n",
    "        if root_dir==None:\n",
    "            root_dir = os.getcwd()+\"/data\"\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wave, _ = torchaudio.load(self.root_dir + \"/\" + self.data[idx], format=\"wav\")\n",
    "        return wave\n",
    "\n",
    "    def get(self,idx):\n",
    "        wave, _ = torchaudio.load(self.root_dir + \"/\" + self.data[idx], format=\"wav\")\n",
    "        return wave\n",
    "    \n",
    "class SortedBatchDataset(Dataset):\n",
    "    def __init__(self, mixed: list, clean: list, batch_size: int = 1):\n",
    "        self.batch_size = batch_size\n",
    "        self.mixed = mixed\n",
    "        self.clean = clean\n",
    "        # if root_dir==None:\n",
    "        #     root_dir = os.getcwd()+\"/data\"\n",
    "        # self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.mixed)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        mixeds = []\n",
    "        cleans = []\n",
    "        max_len = 0\n",
    "        for i in range(self.batch_size):\n",
    "            if idx + i > len(self.mixed): continue\n",
    "            mixed_wave, _ = torchaudio.load(self.mixed[idx*self.batch_size+i])\n",
    "            clean_wave, _ = torchaudio.load(self.clean[idx*self.batch_size+i])\n",
    "            mixed_wave = mixed_wave[0]\n",
    "            clean_wave = clean_wave[0]\n",
    "            assert(mixed_wave.shape[0]==clean_wave.shape[0])\n",
    "            if i==0:\n",
    "                max_len = mixed_wave.shape[0]\n",
    "            else:\n",
    "                mixed_wave = torch.nn.functional.pad(mixed_wave,(0,max_len-(mixed_wave.shape[0])),value=0.0)\n",
    "                clean_wave = torch.nn.functional.pad(clean_wave,(0,max_len-(clean_wave.shape[0])),value=0.0)\n",
    "            # logger.debug(mixed_wave)\n",
    "            mixeds.append(mixed_wave)\n",
    "            cleans.append(clean_wave)\n",
    "        mixeds = np.asarray(mixeds)\n",
    "        cleans = np.asarray(cleans)\n",
    "        return torch.tensor(mixeds).unsqueeze(1), torch.tensor(cleans).unsqueeze(1)\n",
    "\n",
    "    def split(self, val_pct, seed=None):\n",
    "        rnd = random.Random()\n",
    "        if seed is not None:\n",
    "            rnd.seed(seed)\n",
    "        this_len = len(self)\n",
    "        val_batches = math.floor(this_len*val_pct)\n",
    "        val_indices = sorted(rnd.sample(range(this_len), val_batches))\n",
    "        train_mixed = []\n",
    "        train_clean = []\n",
    "        val_mixed = []\n",
    "        val_clean = []\n",
    "        for i in range(this_len):\n",
    "            if i in val_indices:\n",
    "                for x in range(self.batch_size):\n",
    "                    val_mixed.append(self.mixed[i*self.batch_size+x])\n",
    "                    val_clean.append(self.clean[i*self.batch_size+x])\n",
    "            else:\n",
    "                for x in range(self.batch_size):\n",
    "                    train_mixed.append(self.mixed[i*self.batch_size+x])\n",
    "                    train_clean.append(self.clean[i*self.batch_size+x])\n",
    "        \n",
    "        return SortedBatchDataset(train_mixed,train_clean,self.batch_size), SortedBatchDataset(val_mixed,val_clean,self.batch_size)\n",
    "\n",
    "class FrameLoaderEvents(EventEnum):\n",
    "    END_OF_BATCH = \"end_of_batch\"\n",
    "\n",
    "class FrameLoader():\n",
    "    '''Takes a dataloader, frame size and frame shift. It can then be iterated over to produce frames.\\n\n",
    "    Provides padding when sample length would be exceeded.\\n\n",
    "    Returns (mix, clean, has_batch_ended)'''\n",
    "\n",
    "    def __init__(self, dl: DataLoader, frame_size: int, frame_shift: int, engine: Engine | None = None, batch_size=BATCH_SIZE):\n",
    "        self.dl = dl\n",
    "        self.dl_iter = iter(dl)\n",
    "        self.batch_count = len(dl)\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_shift = frame_shift\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_mixed: torch.Tensor\n",
    "        self.batch_clean: torch.Tensor\n",
    "        self.frame_position = 0\n",
    "        self.at_end = True\n",
    "        self.engine = engine\n",
    "    def __iter__(self):\n",
    "        self.dl_iter = iter(self.dl)\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.at_end:\n",
    "            batches: tuple[torch.Tensor,torch.Tensor] = next(self.dl_iter)\n",
    "            self.batch_mixed = batches[0].squeeze_(0)\n",
    "            self.batch_clean = batches[1].squeeze_(0)\n",
    "            self.frame_position = 0\n",
    "            self.at_end = False\n",
    "            # logger.debug(f\"mixed batch shape:{self.batch_mixed.shape} | clean batch shape:{self.batch_clean.shape}\")\n",
    "            # mix_maxes = [torch.max(s[0]) for s in self.batch_mixed]\n",
    "            # clean_maxes = [torch.max(s[0]) for s in self.batch_clean]\n",
    "            # logger.debug(f\"mix_maxes:{str(mix_maxes)} | clean_maxes:{str(clean_maxes)}\")\n",
    "        \n",
    "        frame_end = self.frame_position + self.frame_size\n",
    "        frames = []\n",
    "        for batch_i, batch in enumerate([self.batch_mixed, self.batch_clean]):\n",
    "            shp = batch.shape\n",
    "            frame: torch.Tensor\n",
    "            if frame_end >= shp[2]:\n",
    "                if self.engine is not None and batch_i==0: \n",
    "                    self.engine.fire_event(FrameLoaderEvents.END_OF_BATCH)\n",
    "                self.at_end = True\n",
    "                if frame_end != shp[2]:\n",
    "                    diff = frame_end - shp[2]\n",
    "                    # Pad batch until aligned with frame_end\n",
    "                    frame = torch.zeros((self.batch_size ,1,self.frame_size),dtype=torch.float32)\n",
    "                    frame[:, 0, 0:self.frame_size - diff] = batch[:, 0, self.frame_position:shp[2]]\n",
    "                else:\n",
    "                    frame = torch.zeros((self.batch_size ,1,self.frame_size),dtype=torch.float32)\n",
    "                    frame[:,0,:] = batch[:,0,self.frame_position:frame_end]\n",
    "                    frame = torch.tensor(batch[:,0,self.frame_position:frame_end])\n",
    "            else:\n",
    "                frame = torch.zeros((self.batch_size ,1,self.frame_size),dtype=torch.float32)\n",
    "                frame[:,0,:] = batch[:,0,self.frame_position:frame_end]\n",
    "            frames.append(frame)\n",
    "\n",
    "        self.frame_position += self.frame_shift\n",
    "        # logger.debug(f\"FrameLoader: frame_position[{self.frame_position}] - frame_end[{frame_end}]\")\n",
    "        # perf_logger.debug(f\"Time to load frame at ({self.frame_position}): {time.perf_counter() - start}s\")\n",
    "        if frames[0].shape[2] != self.frame_size:\n",
    "            logger.debug(frames[0].shape[2])\n",
    "            logger.debug(\"FrameLoader issue\")\n",
    "        \n",
    "        return frames[0], frames[1], self.at_end\n",
    "\n",
    "class FrameReconstructor():\n",
    "    '''Constructs a batch of audio samples by continuously adding (batches of) frames to the end of a buffer, excluding overlapping sections.\\n\n",
    "    Use `add_frame()` to return the constructed samples, up to the last batch of frames added.\n",
    "    '''\n",
    "    def __init__(self, frame_size: int, frame_shift: int,target_shape: torch.Size=(BATCH_SIZE, 1, MAXIMUM_SAMPLE_NUM_OF_FRAMES)):\n",
    "        self.audio: torch.Tensor = torch.zeros(target_shape,dtype=torch.float)\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_shift = frame_shift\n",
    "        self.pos: int = 0\n",
    "        self.end: int = frame_size\n",
    "        self.frame_slice_start: int = 0\n",
    "        self.at_end = False\n",
    "    \n",
    "    def add_frame(self, batch: torch.Tensor, _at_end = False):\n",
    "        self.audio[:,0,self.pos:self.end] = batch[:,0,self.frame_slice_start:]\n",
    "\n",
    "        self.pos = self.end\n",
    "        self.end += self.frame_shift\n",
    "        self.frame_slice_start = self.frame_size - self.frame_shift\n",
    "\n",
    "        # if _at_end: \n",
    "        #     return True\n",
    "        # return False\n",
    "\n",
    "        #   Remove if _at_end ends up doing something\n",
    "        return _at_end\n",
    "    \n",
    "    def get_current_audio(self):\n",
    "        out = torch.tensor(self.audio[:,0,:self.end - self.frame_shift]).unsqueeze_(1)\n",
    "        return out\n",
    "\n",
    "    def reset(self):\n",
    "        self.audio = torch.zeros(self.audio.shape,dtype=torch.float)\n",
    "        self.pos = 0\n",
    "        self.end = self.frame_size\n",
    "        self.frame_slice_start = 0\n",
    "        self.at_end = False\n",
    "    \n",
    "\n",
    "_dataset = SortedBatchDataset(get_sequential_wav_paths(\"data/mixed/train\"), get_sequential_wav_paths(\"data/speech_ordered/train\"), batch_size=BATCH_SIZE)\n",
    "train_dataset, val_dataset = _dataset.split(0.2,42)\n",
    "del _dataset\n",
    "base_train_dataloader = DataLoader(train_dataset, shuffle=SHUFFLE)\n",
    "base_val_dataloader = DataLoader(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_dataset))\n",
    "\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "class PESQMetric(Metric):\n",
    "    def __init__(self, stitch_keys=(\"stitch_proc\",\"stitch_clean\"), output_transform = lambda x: x, device=device):\n",
    "        self.stitch_keys=stitch_keys\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().__init__(output_transform, device)\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().reset()\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        if len(output)<=2 or \"stitch_proc\" not in output[2]: return\n",
    "        y_pred: np.ndarray\n",
    "        y: np.ndarray\n",
    "        y_pred, y = output[2][self.stitch_keys[0]].cpu().numpy(), output[2][self.stitch_keys[1]].cpu().numpy()\n",
    "        for i in range(BATCH_SIZE):\n",
    "            self.running_total += calc_pesq(y[i][0], y_pred[i][0])\n",
    "            self.num += 1\n",
    "            # print(\"Processed:\")\n",
    "            # display.display(Audio(y_pred[i][0], rate=16000))\n",
    "            # print(\"Clean:\")\n",
    "            # display.display(Audio(y[i][0], rate=16000))\n",
    "        \n",
    "    @sync_all_reduce(\"num\",\"running_total:SUM\")\n",
    "    def compute(self):\n",
    "        if self.num == 0:\n",
    "            raise NotComputableError(\"PESQ Metric must have one complete sample before computing\")\n",
    "        return self.running_total / self.num\n",
    "\n",
    "class STOIMetric(Metric):\n",
    "    def __init__(self, stitch_keys=(\"stitch_proc\",\"stitch_clean\"), output_transform = lambda x: x, device=device):\n",
    "        self.stitch_keys=stitch_keys\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().__init__(output_transform, device)\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().reset()\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        if len(output)<=2 or \"stitch_proc\" not in output[2]: return\n",
    "        y_pred: np.ndarray\n",
    "        y: np.ndarray\n",
    "        y_pred, y = output[2][self.stitch_keys[0]].cpu().numpy(), output[2][self.stitch_keys[1]].cpu().numpy()\n",
    "        for i in range(BATCH_SIZE):\n",
    "            self.running_total += calc_stoi(y[i][0], y_pred[i][0])\n",
    "            self.num += 1\n",
    "    @sync_all_reduce(\"num\",\"running_total:SUM\")\n",
    "    def compute(self):\n",
    "        if self.num == 0:\n",
    "            raise NotComputableError(\"STOI Metric must have one complete sample before computing\")\n",
    "        return self.running_total / self.num\n",
    "\n",
    "\n",
    "train_metrics: dict[str, Metric] = {\n",
    "    \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "}\n",
    "val_metrics: dict[str, Metric] = {\n",
    "    \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "    \"pesq\": PESQMetric(),\n",
    "    \"stoi\": STOIMetric()\n",
    "}\n",
    "def register_custom_events(eng: Engine):\n",
    "    eng.register_events(*FrameLoaderEvents)\n",
    "\n",
    "def log_trainer_loss(eng: Engine):\n",
    "    iterations = eng.state.iteration % eng.state.iteration_ceiling\n",
    "    print(f\"Epoch[{eng.state.epoch}], Iter[{iterations}] Loss: {eng.state.output}\")\n",
    "\n",
    "def log_custom(eng: Engine, **kwargs):\n",
    "    full_dict = {**eng.state_dict(), \"epoch\": eng.state.epoch, **kwargs}\n",
    "    fmt_string: str = kwargs[\"template\"]\n",
    "    print(fmt_string.format(**full_dict))\n",
    "\n",
    "def log_eval_results(eng: Engine, **kwargs):\n",
    "    prefix = kwargs.get(\"prefix\",\"\")\n",
    "    val_evaluator: Engine = kwargs.get(\"val_evaluator\",None)\n",
    "    val_frame_loader: FrameLoader = kwargs.get(\"val_frame_loader\",None)\n",
    "    if val_evaluator is None:\n",
    "        raise TypeError(\"log_eval_results must be passed the argument `val_evaluator` of type `Engine`\")\n",
    "    if val_frame_loader is None:\n",
    "        raise TypeError(\"log_eval_results must be passed the argument `val_frame_loader` of type `FrameLoader`\")\n",
    "    \n",
    "    val_evaluator.run(val_frame_loader)\n",
    "    metrics = val_evaluator.state.metrics\n",
    "    print(f\"{prefix}Epoch[{eng.state.epoch}] | PESQ:[{metrics['pesq']:.2f}] | STOI:[{metrics['stoi']:.2f}] | Loss:[{metrics['loss']}]\")\n",
    "\n",
    "def set_engine_custom_keys(eng: Engine):\n",
    "    eng.state_dict_user_keys.append(\"iteration_ceiling\")\n",
    "    eng.state.iteration_ceiling = sys.maxsize\n",
    "\n",
    "def set_iteration_ceiling(eng: Engine, *args):\n",
    "    if len(args)==1:\n",
    "        eng.state.iteration_ceiling = args[0]\n",
    "    else:\n",
    "        eng.state.iteration_ceiling = eng.state.iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_GAN:\n",
    "    from models.segan import Generator, Discriminator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveCRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CRN_FRAME_SIZE = 96\n",
    "CRN_FRAME_SHIFT = 40\n",
    "CRN_LR = 1.0e-4\n",
    "\n",
    "CRN_PREP = True\n",
    "CRN_TRAIN = True\n",
    "CRN_LOAD = False\n",
    "CRN_SAVE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:ignite.engine.engine.Engine:Engine run is terminating due to exception: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 89\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m CRN_TRAIN:\n\u001b[1;32m     88\u001b[0m     epchs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 89\u001b[0m     crn_trainer\u001b[38;5;241m.\u001b[39mrun(crn_train_dataloader, max_epochs\u001b[38;5;241m=\u001b[39mepchs)\n\u001b[1;32m     90\u001b[0m     a\u001b[38;5;241m=\u001b[39mpf_train_totals[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(pf_train_num_loops)                        \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     b\u001b[38;5;241m=\u001b[39mpf_train_totals[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(pf_train_num_loops)                        \u001b[38;5;66;03m###\u001b[39;00m\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:889\u001b[0m, in \u001b[0;36mEngine.run\u001b[0;34m(self, data, max_epochs, epoch_length)\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mdataloader \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterrupt_resume_enabled:\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run()\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    891\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_legacy()\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:932\u001b[0m, in \u001b[0;36mEngine._internal_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_as_gen()\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator)\n\u001b[1;32m    933\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m out:\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_run_generator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:990\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine run is terminating due to exception: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_exception(e)\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    993\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:644\u001b[0m, in \u001b[0;36mEngine._handle_exception\u001b[0;34m(self, e)\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mEXCEPTION_RAISED, e)\n\u001b[1;32m    643\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 644\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:956\u001b[0m, in \u001b[0;36mEngine._internal_run_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataloader_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    954\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_engine()\n\u001b[0;32m--> 956\u001b[0m epoch_time_taken \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_once_on_dataset_as_gen()\n\u001b[1;32m    958\u001b[0m \u001b[38;5;66;03m# time is available for handlers but must be updated after fire\u001b[39;00m\n\u001b[1;32m    959\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mtimes[Events\u001b[38;5;241m.\u001b[39mEPOCH_COMPLETED\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m epoch_time_taken\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/ignite/engine/engine.py:1077\u001b[0m, in \u001b[0;36mEngine._run_once_on_dataset_as_gen\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_STARTED)\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n\u001b[0;32m-> 1077\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_function(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mbatch)\n\u001b[1;32m   1078\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fire_event(Events\u001b[38;5;241m.\u001b[39mITERATION_COMPLETED)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_terminate_or_interrupt()\n",
      "Cell \u001b[0;32mIn[14], line 28\u001b[0m, in \u001b[0;36mcrn_train_step\u001b[0;34m(engine, batch)\u001b[0m\n\u001b[1;32m     26\u001b[0m crn_optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m x, y \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), batch[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 28\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m crn_model(x)\n\u001b[1;32m     29\u001b[0m pf_train_totals[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mperf_counter_ns() \u001b[38;5;241m-\u001b[39m pf_train_forward)       \u001b[38;5;66;03m###\u001b[39;00m\n\u001b[1;32m     30\u001b[0m pf_train_back \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter_ns()                                  \u001b[38;5;66;03m###\u001b[39;00m\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/speech_denoising_fyp/models/wavecrn.py:59\u001b[0m, in \u001b[0;36mConvBSRU.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x) \u001b[38;5;66;03m# B,C,D\u001b[39;00m\n\u001b[1;32m     58\u001b[0m output_ \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# D, B, C\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msru(output_) \u001b[38;5;66;03m# D, B, 2C\u001b[39;00m\n\u001b[1;32m     60\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutfc(output) \u001b[38;5;66;03m# D, B, C\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#output = output_ * F.sigmoid(output)\u001b[39;00m\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/modules.py:634\u001b[0m, in \u001b[0;36mSRU.forward\u001b[0;34m(self, input, c0, mask_pad)\u001b[0m\n\u001b[1;32m    632\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rnn \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn_lst:\n\u001b[0;32m--> 634\u001b[0m     h, c \u001b[38;5;241m=\u001b[39m rnn(prevx, c0_[i], mask_pad\u001b[38;5;241m=\u001b[39mmask_pad)\n\u001b[1;32m    635\u001b[0m     prevx \u001b[38;5;241m=\u001b[39m h\n\u001b[1;32m    636\u001b[0m     lstc\u001b[38;5;241m.\u001b[39mappend(c)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/modules.py:276\u001b[0m, in \u001b[0;36mSRUCell.forward\u001b[0;34m(self, input, c0, mask_pad)\u001b[0m\n\u001b[1;32m    273\u001b[0m U, V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_UV(\u001b[38;5;28minput\u001b[39m, c0, mask_pad)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# apply elementwise recurrence to get hidden states h and c\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m h, c \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_recurrence(U, V, residual, c0, scale_val, mask_c, mask_pad)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_after:\n\u001b[1;32m    279\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(h)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/modules.py:298\u001b[0m, in \u001b[0;36mSRUCell.apply_recurrence\u001b[0;34m(self, U, V, residual, c0, scale_val, mask_c, mask_pad)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[0;32m--> 298\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elementwise_recurrence_gpu(U, residual, V, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, c0,\n\u001b[1;32m    299\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_type,\n\u001b[1;32m    300\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    301\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    302\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_skip_term,\n\u001b[1;32m    303\u001b[0m                                           scale_val, mask_c, mask_pad,\n\u001b[1;32m    304\u001b[0m                                           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mamp_recurrence_fp16)\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elementwise_recurrence_naive(U, residual, V, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, c0,\n\u001b[1;32m    307\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_type,\n\u001b[1;32m    308\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    309\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m    310\u001b[0m                                             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_skip_term,\n\u001b[1;32m    311\u001b[0m                                             scale_val, mask_c, mask_pad)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/ops.py:129\u001b[0m, in \u001b[0;36melementwise_recurrence_gpu\u001b[0;34m(U, x, weight_c, bias, c_init, activation_type, hidden_size, bidirectional, has_skip_term, scale_x, dropout_mask_c, mask_pad, amp_recurrence_fp16)\u001b[0m\n\u001b[1;32m    126\u001b[0m scale_x \u001b[38;5;241m=\u001b[39m cast(scale_x) \u001b[38;5;28;01mif\u001b[39;00m scale_x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m scale_x\n\u001b[1;32m    127\u001b[0m dropout_mask_c \u001b[38;5;241m=\u001b[39m cast(dropout_mask_c) \u001b[38;5;28;01mif\u001b[39;00m dropout_mask_c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dropout_mask_c\n\u001b[0;32m--> 129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ElementwiseRecurrence\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m    130\u001b[0m     U,\n\u001b[1;32m    131\u001b[0m     x,\n\u001b[1;32m    132\u001b[0m     weight_c,\n\u001b[1;32m    133\u001b[0m     bias,\n\u001b[1;32m    134\u001b[0m     c_init,\n\u001b[1;32m    135\u001b[0m     activation_type,\n\u001b[1;32m    136\u001b[0m     hidden_size,\n\u001b[1;32m    137\u001b[0m     bidirectional,\n\u001b[1;32m    138\u001b[0m     has_skip_term,\n\u001b[1;32m    139\u001b[0m     scale_x,\n\u001b[1;32m    140\u001b[0m     dropout_mask_c,\n\u001b[1;32m    141\u001b[0m     mask_pad\n\u001b[1;32m    142\u001b[0m )\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/torch/autograd/function.py:575\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    574\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    583\u001b[0m     )\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/cuda_functional.py:176\u001b[0m, in \u001b[0;36mElementwiseRecurrence.forward\u001b[0;34m(ctx, u, x, weight_c, bias, init, activation_type, d_out, bidirectional, has_skip_term, scale_x, mask_c, mask_pad)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# ensure mask_pad is a bool tensor\u001b[39;00m\n\u001b[1;32m    175\u001b[0m mask_pad \u001b[38;5;241m=\u001b[39m mask_pad\u001b[38;5;241m.\u001b[39mbool()\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;28;01mif\u001b[39;00m mask_pad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m h, last_hidden, c \u001b[38;5;241m=\u001b[39m elementwise_recurrence_forward(\n\u001b[1;32m    177\u001b[0m     u,\n\u001b[1;32m    178\u001b[0m     x,\n\u001b[1;32m    179\u001b[0m     weight_c,\n\u001b[1;32m    180\u001b[0m     bias,\n\u001b[1;32m    181\u001b[0m     init,\n\u001b[1;32m    182\u001b[0m     activation_type,\n\u001b[1;32m    183\u001b[0m     d_out,\n\u001b[1;32m    184\u001b[0m     bidirectional,\n\u001b[1;32m    185\u001b[0m     has_skip_term,\n\u001b[1;32m    186\u001b[0m     scale_x,\n\u001b[1;32m    187\u001b[0m     mask_c,\n\u001b[1;32m    188\u001b[0m     mask_pad\n\u001b[1;32m    189\u001b[0m )\n\u001b[1;32m    190\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(u, x, weight_c, bias, init, mask_c, c, mask_pad, scale_x)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h, last_hidden\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if CRN_PREP:\n",
    "    try:\n",
    "        logging.disable(logging.DEBUG)\n",
    "        from models.wavecrn import ConvBSRU\n",
    "        logger.debug(\"wavecrn loaded\")\n",
    "\n",
    "        crn_model: ConvBSRU\n",
    "        crn_optimizer: torch.optim.Adam\n",
    "        if 'crn_model' in locals(): del crn_model\n",
    "        if 'crn_optimizer' in locals(): del crn_optimizer\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        crn_model = ConvBSRU(frame_size=CRN_FRAME_SIZE, conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "        if not CRN_TRAIN:\n",
    "            crn_model.load_state_dict(torch.load(\"saved_models/crn.pt\", weights_only=True))\n",
    "\n",
    "        crn_optimizer = torch.optim.Adam(crn_model.parameters(),lr=CRN_LR)\n",
    "\n",
    "        pf_train_totals = [0,0]                                                     ###\n",
    "        pf_train_num_loops = 0                                                      ###\n",
    "        def crn_train_step(engine, batch):\n",
    "            global pf_train_totals, pf_train_num_loops\n",
    "            pf_train_forward = time.perf_counter_ns()                               ###\n",
    "            crn_model.train()\n",
    "            crn_optimizer.zero_grad()\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            y_pred = crn_model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)       ###\n",
    "            pf_train_back = time.perf_counter_ns()                                  ###\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            crn_optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)          ###\n",
    "            pf_train_num_loops += 1                                                 ###\n",
    "            return loss.item()\n",
    "\n",
    "        crn_trainer = Engine(crn_train_step)\n",
    "        register_custom_events(crn_trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(crn_trainer,'loss')\n",
    "\n",
    "        pbar = ProgressBar()\n",
    "        pbar.attach(crn_trainer,['loss'])\n",
    "\n",
    "        crn_trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        crn_trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "        # crn_trainer.add_event_handler(Events.ITERATION_COMPLETED(every=200),log_trainer_loss)\n",
    "        # crn_trainer.add_event_handler(FrameLoaderEvents.END_OF_BATCH,log_custom,template=\"Batch complete | Epoch: {epoch}, Iteration: {iteration}\")\n",
    "\n",
    "        crn_train_dataloader = FrameLoader(base_train_dataloader, CRN_FRAME_SIZE, CRN_FRAME_SHIFT, crn_trainer)\n",
    "\n",
    "        proc_frame_constructor = FrameReconstructor(CRN_FRAME_SIZE, CRN_FRAME_SHIFT)\n",
    "        clean_frame_constructor = FrameReconstructor(CRN_FRAME_SIZE, CRN_FRAME_SHIFT)\n",
    "\n",
    "        pf_eval_total = 0                                                           ###\n",
    "        pf_eval_num_loops = 0                                                       ###\n",
    "        def crn_val_step(engine, batch):\n",
    "            global pf_eval_total, pf_eval_num_loops\n",
    "            pf_eval_forward = time.perf_counter_ns()                                ###\n",
    "            crn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = crn_model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)         ###\n",
    "                pf_eval_num_loops += 1                                              ###\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "\n",
    "        crn_val_evaluator = Engine(crn_val_step)\n",
    "\n",
    "        # for name, metric in train_metrics.items():\n",
    "        #     metric.attach(crn_train_evaluator, name)\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(crn_val_evaluator, name)\n",
    "\n",
    "        crn_val_dataloader = FrameLoader(base_val_dataloader, CRN_FRAME_SIZE, CRN_FRAME_SHIFT)\n",
    "        crn_trainer.add_event_handler(Events.EPOCH_COMPLETED,log_eval_results,val_evaluator=crn_val_evaluator,val_frame_loader=crn_val_dataloader)\n",
    "\n",
    "        if CRN_TRAIN:\n",
    "            epchs=10\n",
    "            crn_trainer.run(crn_train_dataloader, max_epochs=epchs)\n",
    "            a=pf_train_totals[0] / float(pf_train_num_loops)                        ###\n",
    "            b=pf_train_totals[1] / float(pf_train_num_loops)                        ###\n",
    "            c=pf_eval_total / float(pf_eval_num_loops)                              ###\n",
    "            print(f\"train forward:{a:.20f} | backprop:{b:.20f} | eval forward:{c:.20f}\")\n",
    "        \n",
    "        if CRN_SAVE:\n",
    "            torch.save(crn_model.state_dict(),f\"saved_models/crn_{datetime_string()}.pt\")\n",
    "            write_fstring_file(\"crn\",\"lr:{lr},epochs:{epochs}\",lr=CRN_LR,epochs=epchs)\n",
    "\n",
    "\n",
    "    finally:\n",
    "        logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ns_to_sec(pf_train_totals[0] / float(pf_train_num_loops))                         ###\n",
    "b=ns_to_sec(pf_train_totals[1] / float(pf_train_num_loops))                         ###\n",
    "c=ns_to_sec(pf_eval_total / float(pf_train_num_loops/4))                            ###\n",
    "print(f\"train forward:{a:.20f} | backprop:{b:.20f} | eval forward:{c:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'crn_model' in locals(): torch.save(crn_model.state_dict(),f\"saved_models/crn_{datetime_string()}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RHR-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_FRAME_SIZE = 96\n",
    "RNN_FRAME_SHIFT = 40\n",
    "RNN_LR = 1.0e-5\n",
    "\n",
    "RNN_PREP = True\n",
    "RNN_TRAIN = True\n",
    "RNN_LOAD = False\n",
    "RNN_SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RNN_PREP:\n",
    "    try:\n",
    "        logging.disable(logging.DEBUG)\n",
    "        logger.debug(\"rhrnet loaded\")\n",
    "\n",
    "        import yaml\n",
    "        from models.rhrnetdir.Arg_Parser import Recursive_Parse\n",
    "        from models.rhrnet import RHRNet\n",
    "        rnn_hp = Recursive_Parse(yaml.load(\n",
    "            open('models/rhrnetdir/rhrnet_hyperparameters.yaml', encoding='utf-8'),\n",
    "            Loader=yaml.Loader\n",
    "            ))  \n",
    "        rnn_model: RHRNet\n",
    "        rnn_optimizer: torch.optim.Adam\n",
    "        if 'rnn_model' in locals(): del rnn_model\n",
    "        if 'rnn_optimizer' in locals(): del rnn_optimizer\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        rnn_model = RHRNet(rnn_hp).to(device=device)\n",
    "        if RNN_LOAD:\n",
    "            rnn_model.load_state_dict(torch.load(\"saved_models/rnn.pt\", weights_only=True))\n",
    "\n",
    "        rnn_optimizer = torch.optim.Adam(rnn_model.parameters(),lr=RNN_LR)\n",
    "\n",
    "        pf_train_totals = [0,0]\n",
    "        pf_train_num_loops = 0\n",
    "        def rnn_train_step(engine, batch):\n",
    "            global pf_train_totals, pf_train_num_loops\n",
    "            pf_train_forward = time.perf_counter_ns()\n",
    "            rnn_model.train()\n",
    "            rnn_optimizer.zero_grad()\n",
    "            x, y = batch[0].squeeze_(1).to(device), batch[1].squeeze_(1).to(device)\n",
    "            y_pred = rnn_model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)\n",
    "            pf_train_back = time.perf_counter_ns()\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            rnn_optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)\n",
    "            pf_train_num_loops += 1\n",
    "            return loss.item()\n",
    "\n",
    "        rnn_trainer = Engine(rnn_train_step)\n",
    "        register_custom_events(rnn_trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(rnn_trainer,'loss')\n",
    "\n",
    "        pbar = ProgressBar()\n",
    "        pbar.attach(rnn_trainer,['loss'])\n",
    "\n",
    "        rnn_trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        rnn_trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "        # rnn_trainer.add_event_handler(Events.ITERATION_COMPLETED(every=200),log_trainer_loss)\n",
    "        # rnn_trainer.add_event_handler(FrameLoaderEvents.END_OF_BATCH,log_custom,template=\"Batch complete | Epoch: {epoch}, Iteration: {iteration}\")\n",
    "\n",
    "        rnn_train_dataloader = FrameLoader(base_train_dataloader, RNN_FRAME_SIZE, RNN_FRAME_SHIFT, rnn_trainer)\n",
    "\n",
    "        proc_frame_constructor = FrameReconstructor(RNN_FRAME_SIZE, RNN_FRAME_SHIFT)\n",
    "        clean_frame_constructor = FrameReconstructor(RNN_FRAME_SIZE, RNN_FRAME_SHIFT)\n",
    "\n",
    "        pf_eval_total = 0\n",
    "        pf_eval_num_loops = 0\n",
    "        def rnn_val_step(engine, batch):\n",
    "            global pf_eval_total, pf_eval_num_loops\n",
    "            pf_eval_forward = time.perf_counter_ns()\n",
    "            rnn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].squeeze_(1).to(device), batch[1].squeeze_(1).to(device)\n",
    "                y_pred = rnn_model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)\n",
    "                pf_eval_num_loops += 1\n",
    "                proc_frame_constructor.add_frame(y_pred.unsqueeze_(1))\n",
    "                clean_frame_constructor.add_frame(y.unsqueeze_(1))\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "\n",
    "        rnn_val_evaluator = Engine(rnn_val_step)\n",
    "\n",
    "        # for name, metric in train_metrics.items():\n",
    "        #     metric.attach(rnn_train_evaluator, name)\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(rnn_val_evaluator, name)\n",
    "\n",
    "        rnn_val_dataloader = FrameLoader(base_val_dataloader, RNN_FRAME_SIZE, RNN_FRAME_SHIFT)\n",
    "        rnn_trainer.add_event_handler(Events.EPOCH_COMPLETED,log_eval_results,val_evaluator=rnn_val_evaluator,val_frame_loader=rnn_val_dataloader)\n",
    "\n",
    "        if RNN_TRAIN:\n",
    "            rnn_trainer.run(rnn_train_dataloader, max_epochs=1)\n",
    "            a=pf_train_totals[0] / float(pf_train_num_loops)\n",
    "            b=pf_train_totals[1] / float(pf_train_num_loops)\n",
    "            c=pf_eval_total / float(pf_eval_num_loops)\n",
    "            print(f\"train forward:{a:.20f} | backprop:{b:.20f} | eval forward:{c:.20f}\")\n",
    "\n",
    "        if RNN_SAVE:\n",
    "            torch.save(rnn_model.state_dict(),f\"saved_models/rnn_{datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")}.pt\")\n",
    "\n",
    "\n",
    "    finally:\n",
    "        logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'rnn_model' in locals(): torch.save(rnn_model.state_dict(),f\"saved_models/rnn_{datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wave-U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_FRAME_SIZE = 16153\n",
    "CNN_OUT_FRAME_SIZE = 16009\n",
    "CNN_FRAME_SHIFT = CNN_FRAME_SIZE / 4\n",
    "CNN_LR = 1.0e-4\n",
    "\n",
    "CNN_PREP = True\n",
    "CNN_TRAIN = False\n",
    "CNN_LOAD= False\n",
    "CNN_SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CNN_PREP:\n",
    "    try:\n",
    "        logging.disable(logging.DEBUG)\n",
    "        from models.waveunet import Waveunet\n",
    " \n",
    "        cnn_model: RHRNet\n",
    "        cnn_optimizer: torch.optim.Adam\n",
    "        if 'cnn_model' in locals(): del cnn_model\n",
    "        if 'cnn_optimizer' in locals(): del cnn_optimizer\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        from types import SimpleNamespace\n",
    "        args = SimpleNamespace(\n",
    "            features = 32, \n",
    "            instruments =  [], \n",
    "            res =  \"fixed\", \n",
    "            separate =  0, \n",
    "            channels =  1, \n",
    "            kernel_size =  5, \n",
    "            levels =  3, \n",
    "            depth = 1, \n",
    "            feature_growth =  \"double\",\n",
    "            strides = 4,\n",
    "            conv_type = \"gn\",\n",
    "            sr =  16000,\n",
    "            output_size =  1.0\n",
    "        )\n",
    "        num_features = [args.features*i for i in range(1, args.levels+1)] if args.feature_growth == \"add\" else \\\n",
    "                   [args.features*2**i for i in range(0, args.levels)]\n",
    "        target_outputs = int(args.output_size * args.sr)\n",
    "        cnn_model = Waveunet(args.channels, num_features, args.channels, args.instruments, kernel_size=args.kernel_size,\n",
    "                        target_output_size=target_outputs, depth=args.depth, strides=args.strides,\n",
    "                        conv_type=args.conv_type, res=args.res, separate=args.separate).to(device=device)\n",
    "        if CNN_LOAD:\n",
    "            cnn_model.load_state_dict(torch.load(\"saved_models/cnn.pt\", weights_only=True))\n",
    "\n",
    "        cnn_optimizer = torch.optim.Adam(cnn_model.parameters(),lr=CNN_LR)\n",
    "\n",
    "        pf_train_totals = [0,0]\n",
    "        pf_train_num_loops = 0\n",
    "        def cnn_train_step(engine, batch):\n",
    "            pf_train_forward = time.perf_counter_ns()\n",
    "            cnn_model.train()\n",
    "            cnn_optimizer.zero_grad()\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            y_pred = cnn_model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)\n",
    "            pf_train_back = time.perf_counter_ns()\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            cnn_optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)\n",
    "            pf_train_num_loops += 1\n",
    "            return loss.item()\n",
    "\n",
    "        cnn_trainer = Engine(cnn_train_step)\n",
    "        register_custom_events(cnn_trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(cnn_trainer,'loss')\n",
    "\n",
    "        pbar = ProgressBar()\n",
    "        pbar.attach(cnn_trainer,['loss'])\n",
    "\n",
    "        cnn_trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        cnn_trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "        # cnn_trainer.add_event_handler(Events.ITERATION_COMPLETED(every=200),log_trainer_loss)\n",
    "        # cnn_trainer.add_event_handler(FrameLoaderEvents.END_OF_BATCH,log_custom,template=\"Batch complete | Epoch: {epoch}, Iteration: {iteration}\")\n",
    "\n",
    "        cnn_train_dataloader = FrameLoader(base_train_dataloader, CNN_FRAME_SIZE, CNN_FRAME_SHIFT, cnn_trainer)\n",
    "\n",
    "        proc_frame_constructor = FrameReconstructor(CNN_OUT_FRAME_SIZE, CNN_FRAME_SHIFT)\n",
    "        clean_frame_constructor = FrameReconstructor(CNN_OUT_FRAME_SIZE, CNN_FRAME_SHIFT)\n",
    "\n",
    "        pf_eval_total = 0\n",
    "        pf_eval_num_loops = 0\n",
    "        def cnn_val_step(engine, batch):\n",
    "            pf_eval_forward = time.perf_counter_ns()\n",
    "            cnn_model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = cnn_model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "\n",
    "        cnn_val_evaluator = Engine(cnn_val_step)\n",
    "\n",
    "        # for name, metric in train_metrics.items():\n",
    "        #     metric.attach(cnn_train_evaluator, name)\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(cnn_val_evaluator, name)\n",
    "\n",
    "        cnn_val_dataloader = FrameLoader(base_val_dataloader, CNN_FRAME_SIZE, CNN_FRAME_SHIFT)\n",
    "        cnn_trainer.add_event_handler(Events.EPOCH_COMPLETED,log_eval_results,val_evaluator=cnn_val_evaluator,val_frame_loader=cnn_val_dataloader)\n",
    "\n",
    "        if CNN_TRAIN:\n",
    "            cnn_trainer.run(cnn_train_dataloader, max_epochs=10)\n",
    "\n",
    "        if CNN_SAVE:\n",
    "            torch.save(cnn_model.state_dict(),f\"saved_models/cnn_{datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")}.pt\")\n",
    "\n",
    "    finally:\n",
    "        logging.disable(logging.NOTSET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'cnn_model' in locals(): torch.save(cnn_model.state_dict(),f\"saved_models/cnn_{datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Running Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, l_waveforms: list, r_waveforms:list):\n",
    "        self.l_waveforms = l_waveforms\n",
    "        self.r_waveforms = r_waveforms\n",
    "        super().__init__()\n",
    "    def __len__(self):\n",
    "        return len(self.l_waveforms)\n",
    "    def __getitem__(self,idx):\n",
    "        # return self.l_waveforms[idx], self.r_waveforms[idx]\n",
    "        # print(torch.tensor(self.l_waveforms[idx]).unsqueeze_(0))\n",
    "        # print(torch.tensor(self.l_waveforms[idx]).unsqueeze_(0).shape,flush=True)\n",
    "\n",
    "        return torch.tensor(self.l_waveforms[idx]).unsqueeze_(0), torch.tensor(self.r_waveforms[idx]).unsqueeze_(0)\n",
    "\n",
    "def evaluate_e2e_one_sample(model_dict: dict):\n",
    "    chosen_sample = ntpath.basename(random.choice(glob.glob(\"data/speech_ordered/train/*.wav\")))\n",
    "    clean_sample,_ = torchaudio.load(\"data/speech_ordered/train/\" + chosen_sample)\n",
    "    mixed_sample,_ = torchaudio.load(\"data/mixed/train/\" + chosen_sample)\n",
    "    ds = FakeDataset([mixed_sample],[clean_sample])\n",
    "    dl = DataLoader(ds)\n",
    "    out = {}\n",
    "    for name in model_dict.keys():\n",
    "        out[name] = {}\n",
    "        out[name][\"model\"] = model_dict[name]\n",
    "        match name:\n",
    "            case \"cnn\":\n",
    "                out[name][\"loader\"] = FrameLoader(dl,CNN_FRAME_SIZE,CNN_FRAME_SHIFT, batch_size=1)\n",
    "                out[name][\"processed_constructor\"] = FrameReconstructor(CNN_OUT_FRAME_SIZE,CNN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"clean_constructor\"] = FrameReconstructor(CNN_OUT_FRAME_SIZE,CNN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"in_transform\"] = lambda x: x\n",
    "                out[name][\"out_transform\"] = lambda x: x\n",
    "            case \"rnn\":\n",
    "                out[name][\"loader\"] = FrameLoader(dl,RNN_FRAME_SIZE,RNN_FRAME_SHIFT, batch_size=1)\n",
    "                out[name][\"processed_constructor\"] = FrameReconstructor(RNN_FRAME_SIZE,RNN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"clean_constructor\"] = FrameReconstructor(RNN_FRAME_SIZE,RNN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"in_transform\"] = lambda x: x.squeeze_(1)\n",
    "                out[name][\"out_transform\"] = lambda x: x.unsqueeze_(1)\n",
    "            case \"crn\":\n",
    "                out[name][\"loader\"] = FrameLoader(dl,CRN_FRAME_SIZE,CRN_FRAME_SHIFT, batch_size=1)\n",
    "                out[name][\"processed_constructor\"] = FrameReconstructor(CRN_FRAME_SIZE,CRN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"clean_constructor\"] = FrameReconstructor(CRN_FRAME_SIZE,CRN_FRAME_SHIFT, (1,1,MAXIMUM_SAMPLE_NUM_OF_FRAMES))\n",
    "                out[name][\"in_transform\"] = lambda x: x\n",
    "                out[name][\"out_transform\"] = lambda x: x\n",
    "            # case \"gan\":\n",
    "            #     out[name][\"loader\"] = FrameLoader(dl,GAN_FRAME_SIZE,GAN_FRAME_SHIFT)\n",
    "            case _:\n",
    "                pass\n",
    "        out[name][\"perf\"] = {\"e2e_time\":0, \"avg_forward\":0}\n",
    "    # print(\"shape__:\" + str(clean_sample.shape))\n",
    "    out[\"num_frames\"] = clean_sample.shape[1]\n",
    "    \n",
    "    for name in model_dict.keys():\n",
    "        # print(name)\n",
    "        # print(out[name],flush=True)\n",
    "        t_in: function = out[name][\"in_transform\"]\n",
    "        t_out: function = out[name][\"out_transform\"]\n",
    "        at_end = False\n",
    "        model = out[name][\"model\"]\n",
    "        data = iter(out[name][\"loader\"])\n",
    "        pf_eval_total = 0\n",
    "        n_loops = 0\n",
    "        pf_eval_e2e = time.perf_counter_ns()\n",
    "        while not at_end:\n",
    "            pf_eval_forward = time.perf_counter_ns()\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y, at_end = next(data)\n",
    "                x = t_in(x.to(device=device))\n",
    "                y = t_in(y.to(device=device))\n",
    "                y_pred = model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)\n",
    "                n_loops += 1\n",
    "                try:\n",
    "                    out[name][\"processed_constructor\"].add_frame(t_out(y_pred))\n",
    "                    out[name][\"clean_constructor\"].add_frame(t_out(y))\n",
    "                except RuntimeError as e:\n",
    "                    print(out[name][\"processed_constructor\"].audio.shape)\n",
    "                    raise RuntimeError(e.args)\n",
    "                \n",
    "\n",
    "                if at_end:    #   Frame fully constructed\n",
    "                    y_pred_stitch = out[name][\"processed_constructor\"].get_current_audio()\n",
    "                    y_stitch = out[name][\"clean_constructor\"].get_current_audio()\n",
    "                    print(y_pred_stitch.shape)\n",
    "                    print(y_stitch.shape)\n",
    "                    display.display(Audio(y_pred_stitch,rate=16000))\n",
    "                    display.display(Audio(y_stitch,rate=16000))\n",
    "                    \n",
    "        \n",
    "        pf_eval_e2e = time.perf_counter_ns() - pf_eval_e2e\n",
    "        out[name][\"perf\"][\"e2e_time\"] = pf_eval_e2e\n",
    "        out[name][\"perf\"][\"avg_forward\"] = pf_eval_total / float(n_loops)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/sru/modules.py:538: UserWarning: rnn_dropout > 0 is deprecated and will be removed innext major version of SRU. Please use dropout instead.\n",
      "  warnings.warn(\"rnn_dropout > 0 is deprecated and will be removed in\"\n",
      "/tmp/ipykernel_4100978/410280583.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(self.l_waveforms[idx]).unsqueeze_(0), torch.tensor(self.r_waveforms[idx]).unsqueeze_(0)\n",
      "/tmp/ipykernel_4100978/363489844.py:184: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out = torch.tensor(self.audio[:,0,:self.end - self.frame_shift]).unsqueeze_(1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Array audio input must be a 1D or 2D array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m crn_model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaved_models/crn.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m      3\u001b[0m models \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrn\u001b[39m\u001b[38;5;124m\"\u001b[39m: crn_model }\u001b[38;5;66;03m#, \"rnn\": rnn_model}#, \"cnn\": cnn_model,}\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m out \u001b[38;5;241m=\u001b[39m evaluate_e2e_one_sample(models)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDuration of audio: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_frames\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16000.0\u001b[39m))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mkeys():\n",
      "Cell \u001b[0;32mIn[19], line 84\u001b[0m, in \u001b[0;36mevaluate_e2e_one_sample\u001b[0;34m(model_dict)\u001b[0m\n\u001b[1;32m     82\u001b[0m             y_pred_stitch \u001b[38;5;241m=\u001b[39m out[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessed_constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_current_audio()\n\u001b[1;32m     83\u001b[0m             y_stitch \u001b[38;5;241m=\u001b[39m out[name][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_constructor\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget_current_audio()\n\u001b[0;32m---> 84\u001b[0m             display\u001b[38;5;241m.\u001b[39mdisplay(Audio(y_pred_stitch,rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m))\n\u001b[1;32m     85\u001b[0m             display\u001b[38;5;241m.\u001b[39mdisplay(Audio(y_stitch,rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m))\n\u001b[1;32m     88\u001b[0m pf_eval_e2e \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter_ns() \u001b[38;5;241m-\u001b[39m pf_eval_e2e\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/IPython/lib/display.py:130\u001b[0m, in \u001b[0;36mAudio.__init__\u001b[0;34m(self, data, filename, url, embed, rate, autoplay, normalize, element_id)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrate must be specified when data is a numpy array or list of audio samples.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39m_make_wav(data, rate, normalize)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/IPython/lib/display.py:152\u001b[0m, in \u001b[0;36mAudio._make_wav\u001b[0;34m(data, rate, normalize)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwave\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 152\u001b[0m     scaled, nchan \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39m_validate_and_normalize_with_numpy(data, normalize)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     scaled, nchan \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39m_validate_and_normalize_without_numpy(data, normalize)\n",
      "File \u001b[0;32m/vol/research/FYP_Leo/fyp/lib/python3.12/site-packages/IPython/lib/display.py:183\u001b[0m, in \u001b[0;36mAudio._validate_and_normalize_with_numpy\u001b[0;34m(data, normalize)\u001b[0m\n\u001b[1;32m    181\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mravel()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 183\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArray audio input must be a 1D or 2D array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    185\u001b[0m max_abs_value \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(data))\n\u001b[1;32m    186\u001b[0m normalization_factor \u001b[38;5;241m=\u001b[39m Audio\u001b[38;5;241m.\u001b[39m_get_normalization_factor(max_abs_value, normalize)\n",
      "\u001b[0;31mValueError\u001b[0m: Array audio input must be a 1D or 2D array"
     ]
    }
   ],
   "source": [
    "crn_model = ConvBSRU(frame_size=CRN_FRAME_SIZE, conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "crn_model.load_state_dict(torch.load(\"saved_models/crn.pt\", weights_only=True))\n",
    "models = {  \"crn\": crn_model }#, \"rnn\": rnn_model}#, \"cnn\": cnn_model,}\n",
    "out = evaluate_e2e_one_sample(models)\n",
    "\n",
    "print(\"Duration of audio: \" + str(out[\"num_frames\"] / 16000.0))\n",
    "for name in models.keys():\n",
    "    model_dict = out[name]\n",
    "    print(f\"{name.upper()} -- e2e:{ns_to_sec(model_dict[\"perf\"][\"e2e_time\"])}, average forward:{ns_to_sec(model_dict[\"perf\"][\"avg_forward\"])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
