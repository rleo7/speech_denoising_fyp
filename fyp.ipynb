{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda env update --file environment.yml --prune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:dbg:cuda:0\n",
      "DEBUG:dbg:2.6.0+cu124\n",
      "DEBUG:dbg:NVIDIA RTX A4000\n",
      "DEBUG:dbg:['ffmpeg']\n"
     ]
    }
   ],
   "source": [
    "import os, random, glob, logging, ntpath, math, time, sys, datetime, json, traceback\n",
    "from typing import Callable\n",
    "from IPython import display\n",
    "from IPython.display import Audio\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# pd.options.display.max_seq_items = 2000\n",
    "pd.set_option(\"display.max_colwidth\",None)\n",
    "\n",
    "logging.basicConfig()\n",
    "logger=logging.getLogger(\"dbg\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.disable(logging.NOTSET)\n",
    "perf_logger=logging.getLogger(\"perf\")\n",
    "perf_logger.setLevel(logging.DEBUG)\n",
    "# logging.disable(logging.DEBUG)\n",
    "\n",
    "import torch, torchaudio\n",
    "import torch.nn as nn\n",
    "import torchaudio.functional as audioF\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.debug(device)\n",
    "logger.debug(torch.__version__)\n",
    "logger.debug(torch.cuda.get_device_name(device))\n",
    "logger.debug(torchaudio.list_audio_backends())\n",
    "\n",
    "from ignite.engine import Engine, Events, EventEnum\n",
    "from ignite.metrics import Loss, Metric, RunningAverage\n",
    "from ignite.metrics.metric import reinit__is_reduced, sync_all_reduce\n",
    "from ignite.exceptions import NotComputableError\n",
    "from ignite.handlers.tqdm_logger import ProgressBar\n",
    "from ignite.handlers import Checkpoint, DiskSaver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE = True\n",
    "SAMPLE_RATE = 16000\n",
    "MAXIMUM_SAMPLE_NUM_OF_FRAMES = 640000   #   SAMPLE_RATE*40, i.e. 40 seconds\n",
    "MS_TO_NS = 1e+6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.magic import register_cell_magic\n",
    "from IPython import get_ipython\n",
    "\n",
    "@register_cell_magic\n",
    "def skip(line, cell):\n",
    "    return\n",
    "\n",
    "@register_cell_magic\n",
    "def skip_if(line, cell):\n",
    "    if eval(line):\n",
    "        return\n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystoi\n",
    "import pesq\n",
    "from utils.ssnr import snrseg\n",
    "\n",
    "def combine_audio(speech: torch.Tensor, noise: torch.Tensor, snr: torch.Tensor | int) -> torch.Tensor:\n",
    "    if not (torch.is_floating_point(speech) or torch.is_complex(speech)):\n",
    "        # speech = torch.tensor(speech, dtype=torch.float64, device=speech.device)\n",
    "        speech = speech.to(torch.float64,non_blocking=True)\n",
    "    if not (torch.is_floating_point(noise) or torch.is_complex(noise)):\n",
    "        # noise = torch.tensor(noise, dtype=torch.float64, device=noise.device)\n",
    "        noise = noise.to(torch.float64,non_blocking=True)\n",
    "    if not(type(snr) is torch.Tensor):\n",
    "        snr = torch.tensor([snr])\n",
    "    logger.debug(f\"speech:{speech.ndim}, noise:{noise.ndim}, snr:{snr.ndim}\")\n",
    "    out = audioF.add_noise(speech, noise, snr).to(dtype=torch.float)\n",
    "    return out\n",
    "\n",
    "\n",
    "def calc_snrseg(speech: np.ndarray, processed: np.ndarray) -> float:\n",
    "    v = snrseg(clean_speech=speech, processed_speech=processed, fs=SAMPLE_RATE)\n",
    "    if isinstance(v, np.float64):\n",
    "        v = v.item()\n",
    "    return v\n",
    "\n",
    "def calc_pesq(speech: np.ndarray, processed: np.ndarray) -> float:\n",
    "    return pesq.pesq(ref=speech, deg=processed, fs=SAMPLE_RATE)\n",
    "\n",
    "def calc_stoi(speech: np.ndarray, processed: np.ndarray) -> float:\n",
    "    v = pystoi.stoi(x=speech, y=processed, fs_sig=SAMPLE_RATE)\n",
    "    if isinstance(v, np.float64):\n",
    "        v = v.item()\n",
    "    return v\n",
    "\n",
    "def ns_to_sec(ns: int) -> float:\n",
    "    return ns/1000000000.0\n",
    "\n",
    "def datetime_string() -> str:\n",
    "    return datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")\n",
    "\n",
    "def plot_waveform(waveform, sample_rate=SAMPLE_RATE):\n",
    "    waveform = waveform.numpy()\n",
    "\n",
    "    num_channels, num_frames = waveform.shape\n",
    "    time_axis = torch.arange(0, num_frames) / sample_rate\n",
    "\n",
    "    figure, axes = plt.subplots(num_channels, 1)\n",
    "    if num_channels == 1:\n",
    "        axes = [axes]\n",
    "    for c in range(num_channels):\n",
    "        axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
    "        axes[c].grid(True)\n",
    "        if num_channels > 1:\n",
    "            axes[c].set_ylabel(f\"Channel {c+1}\")\n",
    "    figure.suptitle(\"waveform\")\n",
    "\n",
    "def write_fstring_file(model_name: str, format_string: str, **args):\n",
    "    with open(f\"saved_models/{model_name}_{datetime_string()}.txt\") as f:\n",
    "        f.write(format_string.format(**args))\n",
    "\n",
    "def standardize_batch(batch: torch.Tensor, coerce_func = lambda x: x) -> torch.Tensor:\n",
    "    b = batch.squeeze()\n",
    "    if len(b.shape) == 2:\n",
    "        return b\n",
    "    elif len(b.shape) == 1:\n",
    "        return b.unsqueeze(dim=0)\n",
    "    else:\n",
    "        return coerce_func(b)\n",
    "    \n",
    "def calc_windowing(data_len: int, frame_size: int, frame_shift: int):\n",
    "    num_frames = 0\n",
    "    spare = 0\n",
    "    c = data_len - frame_size\n",
    "    if c < 1:\n",
    "        return 0, 0, 0\n",
    "    num_frames += 1\n",
    "    num_frames += c // frame_shift\n",
    "    spare = c % frame_shift\n",
    "    to_pad = frame_shift - spare\n",
    "    return int(num_frames), int(spare), int(to_pad)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def get_sequential_wav_paths(dir):\n",
    "    count = len(glob.glob(\"*.wav\", root_dir=dir))\n",
    "    lst = []\n",
    "    for i in range(1,count+1):\n",
    "        lst.append(dir + \"/\" + str(i) + \".wav\")\n",
    "    \n",
    "    return lst\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data: list, root_dir: str | None = None):\n",
    "        self.data = data\n",
    "        if root_dir==None:\n",
    "            root_dir = os.getcwd()+\"/data\"\n",
    "        self.root_dir = root_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        wave, _ = torchaudio.load(self.root_dir + \"/\" + self.data[idx], format=\"wav\")\n",
    "        return wave\n",
    "    \n",
    "class SortedBatchDataset(Dataset):\n",
    "    def __init__(self, mixed: list, clean: list, batch_size: int):\n",
    "        self.mixed = mixed\n",
    "        self.clean = clean\n",
    "        self.batch_size = batch_size\n",
    "        # if root_dir==None:\n",
    "        #     root_dir = os.getcwd()+\"/data\"\n",
    "        # self.root_dir = root_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.mixed)/self.batch_size)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        mixeds = []\n",
    "        cleans = []\n",
    "        max_len = 0\n",
    "        for i in range(self.batch_size):\n",
    "            if idx + i > len(self.mixed): continue\n",
    "            mixed_wave, _ = torchaudio.load(self.mixed[idx*self.batch_size+i])\n",
    "            clean_wave, _ = torchaudio.load(self.clean[idx*self.batch_size+i])\n",
    "            mixed_wave = mixed_wave[0]\n",
    "            clean_wave = clean_wave[0]\n",
    "            assert(mixed_wave.shape[0]==clean_wave.shape[0])\n",
    "            if i==0:\n",
    "                max_len = mixed_wave.shape[0]\n",
    "            else:\n",
    "                mixed_wave = torch.nn.functional.pad(mixed_wave,(0,max_len-(mixed_wave.shape[0])),value=0.0)\n",
    "                clean_wave = torch.nn.functional.pad(clean_wave,(0,max_len-(clean_wave.shape[0])),value=0.0)\n",
    "            # logger.debug(mixed_wave)\n",
    "            mixeds.append(mixed_wave)\n",
    "            cleans.append(clean_wave)\n",
    "        mixeds = np.asarray(mixeds)\n",
    "        cleans = np.asarray(cleans)\n",
    "        return torch.tensor(mixeds), torch.tensor(cleans)\n",
    "\n",
    "    def split(self, val_pct, seed=None):\n",
    "        rnd = random.Random(seed)\n",
    "        this_len = len(self)\n",
    "        val_batches = math.floor(this_len*val_pct)\n",
    "        val_indices = sorted(rnd.sample(range(this_len), val_batches))\n",
    "        train_mixed = []\n",
    "        train_clean = []\n",
    "        val_mixed = []\n",
    "        val_clean = []\n",
    "        for i in range(this_len):\n",
    "            if i in val_indices:\n",
    "                for x in range(self.batch_size):\n",
    "                    val_mixed.append(self.mixed[i*self.batch_size+x])\n",
    "                    val_clean.append(self.clean[i*self.batch_size+x])\n",
    "            else:\n",
    "                for x in range(self.batch_size):\n",
    "                    train_mixed.append(self.mixed[i*self.batch_size+x])\n",
    "                    train_clean.append(self.clean[i*self.batch_size+x])\n",
    "        \n",
    "        return SortedBatchDataset(train_mixed,train_clean,self.batch_size), SortedBatchDataset(val_mixed,val_clean,self.batch_size)\n",
    "\n",
    "class FrameLoaderEvents(EventEnum):\n",
    "    END_OF_BATCH = \"end_of_batch\"\n",
    "\n",
    "class FrameLoader():\n",
    "    '''Takes a dataloader, frame size and frame shift. It can then be iterated over to produce frames.\\n\n",
    "    Provides padding when sample length would be exceeded.\\n\n",
    "    Returns (mix, clean, has_batch_ended)'''\n",
    "\n",
    "    def __init__(self, dl: DataLoader, frame_size: int, frame_shift: int, batch_size: int, engine: Engine | None = None, output_transform = lambda x: x):\n",
    "        self.dl = dl\n",
    "        self.dl_iter = iter(dl)\n",
    "        self.batch_count = len(dl)\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_shift = frame_shift\n",
    "        self.batch_size = batch_size\n",
    "        self.batch_mixed: torch.Tensor\n",
    "        self.batch_clean: torch.Tensor\n",
    "        self.frame_position = 0\n",
    "        self.at_end = True\n",
    "        self.engine = engine\n",
    "        self.output_transform = output_transform\n",
    "    def __iter__(self):\n",
    "        self.dl_iter = iter(self.dl)\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.at_end:\n",
    "            batches: tuple[torch.Tensor,torch.Tensor] = next(self.dl_iter)\n",
    "            self.batch_mixed = batches[0].squeeze_()\n",
    "            if len(self.batch_mixed.shape) == 1:\n",
    "                self.batch_mixed.unsqueeze_(0)\n",
    "            self.batch_clean = batches[1].squeeze_()\n",
    "            if len(self.batch_clean.shape) == 1:\n",
    "                self.batch_clean.unsqueeze_(0)\n",
    "            self.frame_position = 0\n",
    "            self.at_end = False\n",
    "            # logger.debug(f\"mixed batch shape:{self.batch_mixed.shape} | clean batch shape:{self.batch_clean.shape}\")\n",
    "            # mix_maxes = [torch.max(s[0]) for s in self.batch_mixed]\n",
    "            # clean_maxes = [torch.max(s[0]) for s in self.batch_clean]\n",
    "            # logger.debug(f\"mix_maxes:{str(mix_maxes)} | clean_maxes:{str(clean_maxes)}\")\n",
    "        \n",
    "        frame_end = self.frame_position + self.frame_size\n",
    "        frames = []\n",
    "        for batch_i, batch in enumerate([self.batch_mixed, self.batch_clean]):\n",
    "            shp = batch.shape\n",
    "            frame: torch.Tensor\n",
    "            if frame_end >= shp[-1]:\n",
    "                if self.engine is not None and batch_i==0: \n",
    "                    self.engine.fire_event(FrameLoaderEvents.END_OF_BATCH)\n",
    "                self.at_end = True\n",
    "                if frame_end != shp[-1]:\n",
    "                    diff = frame_end - shp[-1]\n",
    "                    # Pad batch until aligned with frame_end\n",
    "                    frame = torch.zeros((self.batch_size, self.frame_size), dtype=torch.float32)\n",
    "                    frame[:, 0:self.frame_size - diff] = batch[:, self.frame_position:shp[-1]]\n",
    "                else:\n",
    "                    frame = torch.zeros((self.batch_size, self.frame_size), dtype=torch.float32)\n",
    "                    frame[:, 0:self.frame_size] = batch[:, self.frame_position:frame_end]\n",
    "            else:\n",
    "                frame = torch.zeros((self.batch_size, self.frame_size), dtype=torch.float32)\n",
    "                frame[:, 0:self.frame_size] = batch[:, self.frame_position:frame_end]\n",
    "            frames.append(self.output_transform(frame))\n",
    "\n",
    "        self.frame_position += self.frame_shift\n",
    "        # logger.debug(f\"FrameLoader: frame_position[{self.frame_position}] - frame_end[{frame_end}]\")\n",
    "        # perf_logger.debug(f\"Time to load frame at ({self.frame_position}): {time.perf_counter() - start}s\")\n",
    "\n",
    "        try:\n",
    "\n",
    "            if frames[0].shape[-1] != self.frame_size:\n",
    "                logger.debug(frames[0].shape[-1])\n",
    "                logger.debug(\"FrameLoader issue\")\n",
    "        except Exception as e:\n",
    "            print(frames[0].shape)\n",
    "            raise Exception(e.args)\n",
    "        \n",
    "        return frames[0], frames[1], self.at_end\n",
    "\n",
    "class FrameReconstructor():\n",
    "    '''Constructs a batch of audio samples by continuously adding (batches of) frames to the end of a buffer, excluding overlapping sections.\\n\n",
    "    Use `add_frame()` to return the constructed samples, up to the last batch of frames added.\n",
    "    Use `add_presliced()` when audio data to add should not be treated as an overlapping frame as defined at the constructor's start.\n",
    "    '''\n",
    "    def __init__(self, frame_size: int, frame_shift: int, batch_size: int, output_transform = lambda x: x):\n",
    "        self.audio: torch.Tensor = torch.zeros((batch_size, MAXIMUM_SAMPLE_NUM_OF_FRAMES),dtype=torch.float)\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_shift = frame_shift\n",
    "        self.pos: int = 0\n",
    "        self.end: int = frame_size\n",
    "        self.frame_slice_start: int = 0\n",
    "        self.at_end = False\n",
    "        self.output_transform = output_transform\n",
    "    \n",
    "    def add_frame(self, batch: torch.Tensor, _at_end = False):\n",
    "        '''Adds a frame to the end of currently stored audio. Slices the frame to remove overlap.'''\n",
    "        batch = standardize_batch(batch)\n",
    "        batch = batch.reshape((self.audio.shape[0], batch.shape[-1]))\n",
    "        self.audio[:,self.pos:self.end] = batch[:,self.frame_slice_start:]\n",
    "\n",
    "        self.pos = self.end\n",
    "        self.end += self.frame_shift\n",
    "        self.frame_slice_start = self.frame_size - self.frame_shift\n",
    "\n",
    "        # if _at_end: \n",
    "        #     return True\n",
    "        # return False\n",
    "\n",
    "        #   Remove if _at_end ends up doing something\n",
    "        return _at_end\n",
    "    \n",
    "    def add_presliced(self, batch: torch.Tensor):\n",
    "        '''Appends an arbitrary amount of audio data to the end of currently stored audio.'''\n",
    "        batch = standardize_batch(batch)\n",
    "        batch = batch.reshape((self.audio.shape[0], batch.shape[1]))\n",
    "        self.audio[:,self.pos:self.pos+batch.shape[1]] = batch[:,0:]\n",
    "\n",
    "        self.pos += batch.shape[1]\n",
    "        self.end = self.pos + self.frame_shift\n",
    "    \n",
    "    def get_current_audio(self) -> torch.Tensor:\n",
    "        out = self.audio[:,0:self.end - self.frame_shift].clone().detach()\n",
    "        # out = torch.tensor(self.audio[:,0:self.end - self.frame_shift])\n",
    "        out = self.output_transform(out)\n",
    "        return out\n",
    "\n",
    "    def reset(self):\n",
    "        self.audio = torch.zeros(self.audio.shape,dtype=torch.float)\n",
    "        self.pos = 0\n",
    "        self.end = self.frame_size\n",
    "        self.frame_slice_start = 0\n",
    "        self.at_end = False\n",
    "\n",
    "\n",
    "def get_reference_batch(ds: Dataset, frame_size: int, output_transform=lambda x: x, seed=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    rnd = random.Random(seed)\n",
    "    while True:\n",
    "        idx = rnd.randint(0,len(ds)-1)\n",
    "        batches = ds.__getitem__(idx)\n",
    "        batch, batch2 = batches[0], batches[1]\n",
    "        print(batch.shape)\n",
    "        if batch.shape[-1] < frame_size:\n",
    "            continue\n",
    "        randpos = rnd.randint(0, batch.shape[-1]-frame_size)\n",
    "        batch = batch[:, randpos:randpos+frame_size]\n",
    "        batch2 = batch2[:, randpos:randpos+frame_size]\n",
    "        return output_transform(batch), output_transform(batch2)\n",
    "\n",
    "def get_all_frames(batch: torch.Tensor, frame_size: int, frame_shift: int, output_transform=lambda x: x) -> torch.Tensor:\n",
    "    # logger.debug(f\"before standard: {batch.shape}\")\n",
    "    batch = standardize_batch(batch)\n",
    "    # logger.debug(f\"after standard: {batch.shape}\")\n",
    "    out = batch.unfold(batch.ndim-1, frame_size, frame_shift)\n",
    "    out = output_transform(out)\n",
    "    return out\n",
    "\n",
    "class MultiFrameLoader():\n",
    "    '''Gets a 2D tensor of several overlapped frames. For TCNN.'''\n",
    "    def __init__(self,dl: DataLoader, frame_size: int, frame_shift: int, batch_size: int, num_frames: int, engine: Engine | None = None, output_transform = lambda x: x):\n",
    "        self.dl = dl\n",
    "        self.dl_iter = iter(dl)\n",
    "        self.batch_count = len(dl)\n",
    "        self.frame_size = frame_size\n",
    "        self.frame_shift = frame_shift\n",
    "        self.batch_size = batch_size\n",
    "        self.start_frame = 0\n",
    "        self.num_frames = num_frames\n",
    "        self.batch_mixed: torch.Tensor\n",
    "        self.batch_clean: torch.Tensor\n",
    "        self.at_end = True\n",
    "        self.engine = engine\n",
    "        self.output_transform = output_transform\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self.dl_iter = iter(self.dl)\n",
    "        return self\n",
    "\n",
    "    def __next__(self) -> tuple[torch.Tensor, torch.Tensor, bool]:\n",
    "        if self.at_end:\n",
    "            self.batch_mixed, self.batch_clean = next(self.dl_iter)\n",
    "            self.batch_mixed = get_all_frames(self.batch_mixed, self.frame_size, self.frame_shift)\n",
    "            self.batch_clean = get_all_frames(self.batch_clean, self.frame_size, self.frame_shift)\n",
    "            self.start_frame = 0\n",
    "            self.at_end = False\n",
    "        spare = self.batch_mixed.shape[1] - (self.start_frame + self.num_frames)\n",
    "        if spare <= 0:\n",
    "            self.at_end = True\n",
    "            if spare == 0:\n",
    "                _num_frames = self.num_frames\n",
    "            else:\n",
    "                _num_frames = self.num_frames + spare   #   i.e. self.num_frames - abs(spare)\n",
    "        else:\n",
    "            _num_frames = self.num_frames\n",
    "        out = (self.output_transform(self.batch_mixed.narrow(1, self.start_frame, _num_frames)),\n",
    "                self.output_transform(self.batch_clean.narrow(1, self.start_frame, _num_frames)),\n",
    "                self.at_end)\n",
    "        self.start_frame += _num_frames\n",
    "        return out\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.MSELoss()\n",
    "\n",
    "pf_train_totals = [0,0]                                                     ###\n",
    "pf_train_num_loops = 0                                                      ###\n",
    "pf_eval_total = 0\n",
    "pf_eval_num_loops = 0\n",
    "\n",
    "class PESQMetric(Metric):\n",
    "    def __init__(self, stitch_keys=(\"stitch_proc\",\"stitch_clean\"), output_transform = lambda x: x, device=device):\n",
    "        self.stitch_keys=stitch_keys\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().__init__(output_transform, device)\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().reset()\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        if len(output)<=2 or \"stitch_proc\" not in output[2]: return\n",
    "        y_pred: np.ndarray = standardize_batch(output[2][self.stitch_keys[0]]).cpu().numpy()\n",
    "        y: np.ndarray = standardize_batch(output[2][self.stitch_keys[1]]).cpu().numpy()\n",
    "        for i in range(y.shape[0]):\n",
    "            self.running_total += calc_pesq(y[i], y_pred[i])\n",
    "            self.num += 1\n",
    "        \n",
    "    @sync_all_reduce(\"num\",\"running_total:SUM\")\n",
    "    def compute(self):\n",
    "        if self.num == 0:\n",
    "            raise NotComputableError(\"PESQ Metric must have one complete sample before computing\")\n",
    "        return self.running_total / self.num\n",
    "\n",
    "class STOIMetric(Metric):\n",
    "    def __init__(self, stitch_keys=(\"stitch_proc\",\"stitch_clean\"), output_transform = lambda x: x, device=device):\n",
    "        self.stitch_keys=stitch_keys\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().__init__(output_transform, device)\n",
    "    @reinit__is_reduced\n",
    "    def reset(self):\n",
    "        self.running_total=0.0\n",
    "        self.num=0\n",
    "        super().reset()\n",
    "    @reinit__is_reduced\n",
    "    def update(self, output):\n",
    "        if len(output)<=2 or \"stitch_proc\" not in output[2]: return\n",
    "        y_pred: np.ndarray = standardize_batch(output[2][self.stitch_keys[0]]).cpu().numpy()\n",
    "        y: np.ndarray = standardize_batch(output[2][self.stitch_keys[1]]).cpu().numpy()\n",
    "        for i in range(y.shape[0]):\n",
    "            self.running_total += calc_stoi(y[i], y_pred[i])\n",
    "            self.num += 1\n",
    "    @sync_all_reduce(\"num\",\"running_total:SUM\")\n",
    "    def compute(self):\n",
    "        if self.num == 0:\n",
    "            raise NotComputableError(\"STOI Metric must have one complete sample before computing\")\n",
    "        return self.running_total / self.num\n",
    "\n",
    "\n",
    "class ValidationEvents(EventEnum):\n",
    "    VALIDATION_COMPLETED = \"validation_completed\"\n",
    "\n",
    "def register_custom_events(eng: Engine):\n",
    "    eng.register_events(*FrameLoaderEvents)\n",
    "    eng.register_events(*ValidationEvents)\n",
    "\n",
    "def log_trainer_loss(eng: Engine):\n",
    "    iterations = eng.state.iteration % eng.state.iteration_ceiling\n",
    "    print(f\"Epoch[{eng.state.epoch}], Iter[{iterations}] Loss: {eng.state.output}\")\n",
    "\n",
    "def log_custom(eng: Engine, **kwargs):\n",
    "    full_dict = {**eng.state_dict(), \"epoch\": eng.state.epoch, **kwargs}\n",
    "    fmt_string: str = kwargs[\"template\"]\n",
    "    print(fmt_string.format(**full_dict))\n",
    "\n",
    "def run_eval(eng: Engine, **kwargs):\n",
    "    validator: Engine = kwargs.get(\"validator\",None)\n",
    "    val_frame_loader: FrameLoader = kwargs.get(\"val_frame_loader\",None)\n",
    "    if validator is None:\n",
    "        raise TypeError(\"log_eval_results must be passed the argument `validator` of type `Engine`\")\n",
    "    if val_frame_loader is None:\n",
    "        raise TypeError(\"log_eval_results must be passed the argument `val_frame_loader` of type `FrameLoader`\")\n",
    "    \n",
    "    validator.run(val_frame_loader)\n",
    "    eng.fire_event(ValidationEvents.VALIDATION_COMPLETED)\n",
    "\n",
    "def log_eval_results(eng: Engine, **kwargs):\n",
    "    prefix = kwargs.get(\"prefix\",\"\")\n",
    "    validator: Engine = kwargs.get(\"validator\",None)\n",
    "    if validator is None:\n",
    "        raise TypeError(\"log_eval_results must be passed the argument `validator` of type `Engine`\")\n",
    "    \n",
    "    metrics = validator.state.metrics\n",
    "    metrics_out = kwargs.get(\"metrics_out\",None)\n",
    "    if metrics_out != None:\n",
    "        metrics_out.append(metrics.copy())\n",
    "    print(f\"{prefix}Epoch[{eng.state.epoch}] | PESQ:[{metrics['pesq']:.2f}] | STOI:[{metrics['stoi']:.2f}] | Loss:[{metrics['loss']}]\")\n",
    "\n",
    "def set_engine_custom_keys(eng: Engine):\n",
    "    eng.state_dict_user_keys.append(\"iteration_ceiling\")\n",
    "    eng.state.iteration_ceiling = sys.maxsize\n",
    "\n",
    "def set_iteration_ceiling(eng: Engine, *args):\n",
    "    if len(args)==1:\n",
    "        eng.state.iteration_ceiling = args[0]\n",
    "    else:\n",
    "        eng.state.iteration_ceiling = eng.state.iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_hp = {\n",
    "    \"frame_size\":16384,\n",
    "    \"frame_shift\":8192,\n",
    "    \"g_lr\":1.0e-4,\n",
    "    \"d_lr\":1.0e-4,\n",
    "    \"batch_size\":128,\n",
    "    \"epochs\":80,\n",
    "    \"save\":True,\n",
    "    \"load\":None,\n",
    "    \"model_type\":\"gan\",\n",
    "}\n",
    "\n",
    "GAN_RUN_ON_LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan(hp: dict = gan_hp):\n",
    "    global pf_train_totals, pf_train_num_loops, pf_eval_total, pf_eval_num_loops\n",
    "    pf_train_totals = [0,0]\n",
    "    pf_train_num_loops = 0\n",
    "    pf_eval_total = 0\n",
    "    pf_eval_num_loops = 0\n",
    "    try:\n",
    "        datestring_at_start = datetime_string()\n",
    "        os.mkdir(f\"saved_models/gan_{datestring_at_start}\")\n",
    "\n",
    "        from models.segan import Discriminator, Generator\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        out = {\"hp\": hp}\n",
    "        gen = Generator().to(device=device)\n",
    "        dcrim = Discriminator().to(device=device)\n",
    "\n",
    "        if hp[\"load\"] != None:\n",
    "            gen.load_state_dict(torch.load(hp[\"load\"][0], weights_only=True))\n",
    "            dcrim.load_state_dict(torch.load(hp[\"load\"][1], weights_only=True))\n",
    "        \n",
    "        g_optimizer = torch.optim.RMSprop(gen.parameters(), lr=hp[\"g_lr\"])\n",
    "        d_optimizer = torch.optim.RMSprop(dcrim.parameters(), lr=hp[\"d_lr\"])\n",
    "        criterion = nn.L1Loss()\n",
    "        out[\"optimizer\"] = str(g_optimizer).split(\"(\")[0]\n",
    "        out[\"criterion\"] = str(criterion).split(\"(\")[0]\n",
    "\n",
    "\n",
    "        _dataset = SortedBatchDataset(get_sequential_wav_paths(\"data/mixed/train\"),\n",
    "                                    get_sequential_wav_paths(\"data/speech_ordered/train\"), \n",
    "                                    batch_size=hp[\"batch_size\"])\n",
    "        train_dataset, val_dataset = _dataset.split(0.2)\n",
    "        del _dataset\n",
    "        base_train_dataloader = DataLoader(train_dataset, shuffle=SHUFFLE)\n",
    "        base_val_dataloader = DataLoader(val_dataset)\n",
    "        r = get_reference_batch(train_dataset, hp[\"frame_size\"], lambda x: x.view(hp[\"batch_size\"],1,-1))\n",
    "        ref_batch = torch.cat((r[0],r[1]),dim=1).to(device=device)\n",
    "        z = torch.zeros((hp[\"batch_size\"],1024,8)).to(device=device)\n",
    "        print(ref_batch.shape)\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            # global pf_train_totals, pf_train_num_loops\n",
    "            # pf_train_forward = time.perf_counter_ns()                               ###\n",
    "            dcrim.train()\n",
    "            dcrim.zero_grad()\n",
    "            x, y = batch[0].to(device=device), batch[1].to(device=device)\n",
    "            nn.init.normal_(z)\n",
    "            combined_batch = torch.cat((x.clone().detach(),y.clone().detach()),dim=1)\n",
    "            output = dcrim(combined_batch, ref_batch)\n",
    "            clean_loss = torch.mean((output - 1.0) ** 2)\n",
    "            clean_loss.backward()\n",
    "\n",
    "            gen_out = gen(x, z)\n",
    "            output = dcrim(torch.cat((gen_out, x), dim=1),ref_batch)\n",
    "            noisy_loss = torch.mean(output ** 2)\n",
    "            noisy_loss.backward()\n",
    "\n",
    "            d_optimizer.step()\n",
    "\n",
    "            gen.train()\n",
    "            gen.zero_grad()\n",
    "            gen_out = gen(x, z)\n",
    "            gen_noise_pair = torch.cat((gen_out, x), dim=1)\n",
    "            output = dcrim(gen_noise_pair, ref_batch)\n",
    "\n",
    "            g_loss_ = 0.5 * torch.mean((output - 1.0) ** 2)\n",
    "            l1_dist = torch.abs(torch.add(gen_out, torch.neg(y)))\n",
    "            g_cond_loss = 100 * torch.mean(l1_dist)\n",
    "            g_loss = g_loss_ + g_cond_loss\n",
    "            g_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            return g_loss.item()\n",
    "            \n",
    "        trainer = Engine(train_step)\n",
    "        register_custom_events(trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')\n",
    "        pbar = ProgressBar(desc=\"Training Epoch\")\n",
    "        pbar.attach(trainer,['loss'])\n",
    "\n",
    "        trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "\n",
    "        train_dataloader = FrameLoader(base_train_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"],\n",
    "                                        hp[\"batch_size\"], engine=trainer, \n",
    "                                        output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"],\n",
    "                                                    output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"],\n",
    "                                                     output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        \n",
    "        def val_step(engine, batch):\n",
    "            with torch.no_grad():\n",
    "                # global pf_eval_total, pf_eval_num_loops\n",
    "                x, y = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                nn.init.normal_(z)\n",
    "                y_pred = gen(x, z)\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "        \n",
    "        validator = Engine(val_step)\n",
    "        pbar = ProgressBar(desc=\"Validation\")\n",
    "        pbar.attach(validator,['loss'])\n",
    "        val_metrics: dict[str, Metric] = {\n",
    "            \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "            \"pesq\": PESQMetric(),\n",
    "            \"stoi\": STOIMetric()\n",
    "        }\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(validator, name)\n",
    "\n",
    "        checkpoint_to_save = {\"gen\": gen, \"dcrim\": dcrim}\n",
    "        checkpoint_handler = Checkpoint(\n",
    "            checkpoint_to_save, f\"saved_models/gan_{datestring_at_start}\",\n",
    "            filename_prefix=\"best\", score_function=lambda eng: eng.state.metrics['pesq'],n_saved=2\n",
    "        )\n",
    "\n",
    "        metrics_out = []\n",
    "        out[\"metrics\"] = metrics_out\n",
    "        val_dataloader = FrameLoader(base_val_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"],\n",
    "                                     hp[\"batch_size\"], output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED,run_eval,validator=validator,val_frame_loader=val_dataloader)\n",
    "        trainer.add_event_handler(ValidationEvents.VALIDATION_COMPLETED,log_eval_results,validator=validator,metrics_out=metrics_out)\n",
    "        validator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n",
    "            \n",
    "        time_train = time.perf_counter_ns()\n",
    "        trainer.run(train_dataloader, max_epochs=hp[\"epochs\"])\n",
    "        out[\"total_time\"] = time.perf_counter_ns() - time_train\n",
    "        # out[\"fwd\"]=pf_train_totals[0] / float(pf_train_num_loops)                        ###\n",
    "        # out[\"bck\"]=pf_train_totals[1] / float(pf_train_num_loops)                        ###\n",
    "        # out[\"eval\"]=pf_eval_total / float(pf_eval_num_loops)                          ###\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"failed\")\n",
    "        print(traceback.format_exc())\n",
    "        print(e)\n",
    "    \n",
    "    finally:\n",
    "        if \"dcrim\" in locals():\n",
    "            if hp[\"save\"]:\n",
    "                torch.save(gen.state_dict(),f\"saved_models/gan_{datestring_at_start}/gen_final.pt\")\n",
    "                torch.save(dcrim.state_dict(),f\"saved_models/gan_{datestring_at_start}/dcrim_final.pt\")\n",
    "                with open(f\"saved_models/gan_{datestring_at_start}/out.json\",\"w\") as f:\n",
    "                    json.dump({k: out[k] for k in out.keys() - {'gen', 'dcrim'}},f)\n",
    "            return out\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "if GAN_RUN_ON_LOAD:\n",
    "    gan()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WaveCRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crn_hp = {\n",
    "    \"frame_size\":96,\n",
    "    \"frame_shift\":40,\n",
    "    \"lr\":2.0e-5,\n",
    "    \"batch_size\":128,\n",
    "    \"epochs\":80,\n",
    "    \"save\":True,\n",
    "    \"load\":None,\n",
    "    \"model_type\":\"crn\",\n",
    "}\n",
    "\n",
    "CRN_RUN_ON_LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crn(hp: dict = crn_hp):\n",
    "    global pf_train_totals, pf_train_num_loops, pf_eval_total, pf_eval_num_loops\n",
    "    pf_train_totals = [0,0]                                                     ###\n",
    "    pf_train_num_loops = 0                                                      ###\n",
    "    pf_eval_total = 0\n",
    "    pf_eval_num_loops = 0\n",
    "    try:\n",
    "        datestring_at_start = datetime_string()\n",
    "        os.mkdir(f\"saved_models/crn_{datestring_at_start}\")\n",
    "\n",
    "        from models.wavecrn import ConvBSRU\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        out = {\"hp\": hp}\n",
    "        model = ConvBSRU(frame_size=hp[\"frame_size\"], conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "        if hp[\"load\"] != None:\n",
    "            model.load_state_dict(torch.load(hp[\"load\"], weights_only=True))\n",
    "        out[\"model\"] = model\n",
    "        \n",
    "        # optimizer = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "        criterion = nn.L1Loss()\n",
    "        out[\"optimizer\"] = str(optimizer).split(\"(\")[0]\n",
    "        out[\"criterion\"] = str(criterion).split(\"(\")[0]\n",
    "\n",
    "        _dataset = SortedBatchDataset(get_sequential_wav_paths(\"data/mixed/train\"), \n",
    "                                      get_sequential_wav_paths(\"data/speech_ordered/train\"), \n",
    "                                      batch_size=hp[\"batch_size\"])\n",
    "        train_dataset, val_dataset = _dataset.split(0.2)\n",
    "        del _dataset\n",
    "        base_train_dataloader = DataLoader(train_dataset, shuffle=SHUFFLE)\n",
    "        base_val_dataloader = DataLoader(val_dataset)\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            global pf_train_totals, pf_train_num_loops\n",
    "            pf_train_forward = time.perf_counter_ns()                               ###\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            y_pred = model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)       ###\n",
    "            pf_train_back = time.perf_counter_ns()                                  ###\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)          ###\n",
    "            pf_train_num_loops += 1                                                 ###\n",
    "            return loss.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "        register_custom_events(trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')\n",
    "        pbar = ProgressBar(desc=\"Training Epoch\")\n",
    "        pbar.attach(trainer,['loss'])\n",
    "\n",
    "        trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "\n",
    "        train_dataloader = FrameLoader(base_train_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"], batch_size=hp[\"batch_size\"], engine=trainer, output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "\n",
    "        \n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"], output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"], output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        def val_step(engine, batch):\n",
    "            global pf_eval_total, pf_eval_num_loops\n",
    "            pf_eval_forward = time.perf_counter_ns()                                ###\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)         ###\n",
    "                pf_eval_num_loops += 1                                              ###\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "\n",
    "        validator = Engine(val_step)\n",
    "        pbar = ProgressBar(desc=\"Validation\")\n",
    "        pbar.attach(validator,['loss'])\n",
    "        val_metrics: dict[str, Metric] = {\n",
    "            \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "            \"pesq\": PESQMetric(),\n",
    "            \"stoi\": STOIMetric()\n",
    "        }\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(validator, name)\n",
    "\n",
    "        checkpoint_to_save = {\"model\":model}\n",
    "        checkpoint_handler = Checkpoint(\n",
    "            checkpoint_to_save, f\"saved_models/crn_{datestring_at_start}\",\n",
    "            filename_prefix=\"best\", score_function=lambda eng: eng.state.metrics['pesq'],n_saved=2\n",
    "        )\n",
    "\n",
    "        metrics_out = []\n",
    "        out[\"metrics\"] = metrics_out\n",
    "        val_dataloader = FrameLoader(base_val_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"],\n",
    "                                     hp[\"batch_size\"],output_transform=lambda x: x.view((hp[\"batch_size\"],1,-1)))\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED,run_eval,validator=validator,val_frame_loader=val_dataloader)\n",
    "        trainer.add_event_handler(ValidationEvents.VALIDATION_COMPLETED,log_eval_results,validator=validator,metrics_out=metrics_out)\n",
    "        validator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n",
    "\n",
    "\n",
    "        time_train = time.perf_counter_ns()\n",
    "        trainer.run(train_dataloader, max_epochs=hp[\"epochs\"])\n",
    "        out[\"total_time\"] = time.perf_counter_ns() - time_train\n",
    "        out[\"fwd\"]=pf_train_totals[0] / float(pf_train_num_loops)                        ###\n",
    "        out[\"bck\"]=pf_train_totals[1] / float(pf_train_num_loops)                        ###\n",
    "        out[\"eval\"]=pf_eval_total / float(pf_eval_num_loops)                          ###\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.print_exc())\n",
    "        print(e)    \n",
    "    finally:\n",
    "        if \"model\" in locals():\n",
    "            if hp[\"save\"]:\n",
    "                torch.save(model.state_dict(),f\"saved_models/crn_{datestring_at_start}/final.pt\")\n",
    "                json_dict = {k: out[k] for k in out.keys() - {'model'}}\n",
    "                with open(f\"saved_models/crn_{datestring_at_start}/out.json\",\"w\") as f:\n",
    "                    json.dump(json_dict,f)\n",
    "            return out\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "if CRN_RUN_ON_LOAD:\n",
    "    crn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'crn_model' in locals(): torch.save(crn_model.state_dict(),f\"saved_models/crn_{datetime_string()}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RHR-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_hp = {\n",
    "    \"frame_size\":1024,\n",
    "    \"frame_shift\":256,\n",
    "    \"lr\":1.0e-4,\n",
    "    \"batch_size\":128,\n",
    "    \"epochs\":30,\n",
    "    \"save\":True,\n",
    "    \"load\":None,\n",
    "    \"model_type\":\"rnn\",\n",
    "}\n",
    "\n",
    "RNN_RUN_ON_LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(hp: dict = rnn_hp):\n",
    "    global pf_train_totals, pf_train_num_loops, pf_eval_total, pf_eval_num_loops\n",
    "    pf_train_totals = [0,0]                                                     ###\n",
    "    pf_train_num_loops = 0                                                      ###\n",
    "    pf_eval_total = 0\n",
    "    pf_eval_num_loops = 0\n",
    "    try:\n",
    "        datestring_at_start = datetime_string()\n",
    "        os.mkdir(f\"saved_models/rnn_{datestring_at_start}\")\n",
    "        \n",
    "        import yaml\n",
    "        from models.rhrnetdir.Arg_Parser import Recursive_Parse\n",
    "        from models.rhrnet import RHRNet\n",
    "        rnn_hp = Recursive_Parse(yaml.load(\n",
    "            open('models/rhrnetdir/rhrnet_hyperparameters.yaml', encoding='utf-8'),\n",
    "            Loader=yaml.Loader\n",
    "            ))  \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        out = {\"hp\": hp, \"datetime_str\": datestring_at_start}\n",
    "        model = RHRNet(rnn_hp).to(device=device)\n",
    "        if hp[\"load\"] != None:\n",
    "            model.load_state_dict(torch.load(hp[\"load\"], weights_only=True))\n",
    "        out[\"model\"] = model\n",
    "        \n",
    "        optimizer = torch.optim.RMSprop(model.parameters(),lr=hp[\"lr\"])\n",
    "        criterion = nn.L1Loss()\n",
    "        out[\"optimizer\"] = str(optimizer).split(\"(\")[0]\n",
    "        out[\"criterion\"] = str(criterion).split(\"(\")[0]\n",
    "\n",
    "        _dataset = SortedBatchDataset(get_sequential_wav_paths(\"data/mixed/train\"), get_sequential_wav_paths(\"data/speech_ordered/train\"), batch_size=hp[\"batch_size\"])\n",
    "        train_dataset, val_dataset = _dataset.split(0.2)\n",
    "        del _dataset\n",
    "        base_train_dataloader = DataLoader(train_dataset, shuffle=SHUFFLE)\n",
    "        base_val_dataloader = DataLoader(val_dataset)\n",
    "        print(f\"val dataset:{len(val_dataset)}\")\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            global pf_train_totals, pf_train_num_loops\n",
    "            pf_train_forward = time.perf_counter_ns()                               ###\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            y_pred = model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)       ###\n",
    "            pf_train_back = time.perf_counter_ns()                                  ###\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)          ###\n",
    "            pf_train_num_loops += 1                                                 ###\n",
    "            return loss.item()\n",
    "\n",
    "        trainer = Engine(train_step)\n",
    "        register_custom_events(trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')\n",
    "        pbar = ProgressBar(desc=\"Training Epoch\")\n",
    "        pbar.attach(trainer,['loss'])\n",
    "\n",
    "        trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "\n",
    "        train_dataloader = FrameLoader(base_train_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"], batch_size=hp[\"batch_size\"], engine=trainer, output_transform=lambda x: x.view((hp[\"batch_size\"],-1)))\n",
    "\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"], output_transform=lambda x: x.view((hp[\"batch_size\"],-1)))\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"], output_transform=lambda x: x.view((hp[\"batch_size\"],-1)))\n",
    "        def val_step(engine, batch):\n",
    "            global pf_eval_total, pf_eval_num_loops\n",
    "            pf_eval_forward = time.perf_counter_ns()                                ###\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)         ###\n",
    "                pf_eval_num_loops += 1                                              ###\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "\n",
    "        validator = Engine(val_step)\n",
    "        pbar = ProgressBar(desc=\"Validation\")\n",
    "        pbar.attach(validator,['loss'])\n",
    "        val_metrics: dict[str, Metric] = {\n",
    "            \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "            \"pesq\": PESQMetric(),\n",
    "            \"stoi\": STOIMetric()\n",
    "        }\n",
    "\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(validator, name)\n",
    "\n",
    "        checkpoint_to_save = {\"model\":model}\n",
    "        checkpoint_handler = Checkpoint(\n",
    "            checkpoint_to_save, f\"saved_models/rnn_{datestring_at_start}\",\n",
    "            filename_prefix=\"best\", score_function=lambda eng: eng.state.metrics['pesq'],n_saved=2\n",
    "        )\n",
    "\n",
    "        metrics_out = []\n",
    "        out[\"metrics\"] = metrics_out\n",
    "        val_dataloader = FrameLoader(base_val_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED,run_eval,validator=validator,val_frame_loader=val_dataloader)\n",
    "        trainer.add_event_handler(ValidationEvents.VALIDATION_COMPLETED,log_eval_results,validator=validator,metrics_out=metrics_out)\n",
    "        validator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n",
    "\n",
    "        time_train = time.perf_counter_ns()\n",
    "        trainer.run(train_dataloader, max_epochs=hp[\"epochs\"])\n",
    "        out[\"total_time\"] = time.perf_counter_ns() - time_train\n",
    "        out[\"fwd\"]=pf_train_totals[0] / float(pf_train_num_loops)                        ###\n",
    "        out[\"bck\"]=pf_train_totals[1] / float(pf_train_num_loops)                        ###\n",
    "        out[\"eval\"]=pf_eval_total / float(pf_eval_num_loops)                          ###\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(e)    \n",
    "    finally:\n",
    "        if \"model\" in locals():\n",
    "            if hp[\"save\"]:\n",
    "                torch.save(model.state_dict(),f\"saved_models/rnn_{datestring_at_start}/final.pt\")\n",
    "                with open(f\"saved_models/rnn_{datestring_at_start}/out.json\",\"w\") as f:\n",
    "                    json.dump({k: out[k] for k in out.keys() - {'model'}},f)\n",
    "            return out\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "if RNN_RUN_ON_LOAD:\n",
    "    rnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=ns_to_sec(pf_train_totals[0] / float(pf_train_num_loops))                         ###\n",
    "# b=ns_to_sec(pf_train_totals[1] / float(pf_train_num_loops))                         ###\n",
    "# c=ns_to_sec(pf_eval_total / float(pf_train_num_loops/4))                            ###\n",
    "# print(f\"train forward:{a:.20f} | backprop:{b:.20f} | eval forward:{c:.20f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if 'rnn_model' in locals(): torch.save(rnn_model.state_dict(),f\"saved_models/rnn_{datetime.datetime.now().strftime(\"%d-%m-%Y--%H-%M-%S\")}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_hp = {\n",
    "    \"frame_size\":320,\n",
    "    \"frame_shift\":160,\n",
    "    \"num_frames\": 300,\n",
    "    \"lr\":1.0e-3,\n",
    "    \"batch_size\":16,\n",
    "    \"epochs\":30,\n",
    "    \"save\":False,\n",
    "    \"load\":None,\n",
    "    \"model_type\":\"cnn\",\n",
    "}\n",
    "\n",
    "CNN_RUN_ON_LOAD = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(hp: dict = cnn_hp):\n",
    "    global pf_train_totals, pf_train_num_loops, pf_eval_total, pf_eval_num_loops\n",
    "    pf_train_totals = [0,0]                                                     ###\n",
    "    pf_train_num_loops = 0                                                      ###\n",
    "    pf_eval_total = 0\n",
    "    pf_eval_num_loops = 0\n",
    "    logging.disable(logging.DEBUG)\n",
    "    try:\n",
    "        datestring_at_start = datetime_string()\n",
    "        os.mkdir(f\"saved_models/cnn_{datestring_at_start}\")\n",
    "\n",
    "        from models.tcnn import TCNN\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        out = {\"hp\": hp}\n",
    "        model = TCNN().to(device=device)\n",
    "        if hp[\"load\"] != None:\n",
    "            model.load_state_dict(torch.load(hp[\"load\"], weights_only=True))\n",
    "        out[\"model\"] = model\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "        criterion = nn.L1Loss()\n",
    "        out[\"optimizer\"] = str(optimizer).split(\"(\")[0]\n",
    "        out[\"criterion\"] = str(criterion).split(\"(\")[0]\n",
    "\n",
    "        _dataset = SortedBatchDataset(get_sequential_wav_paths(\"data/mixed/train\"), \n",
    "                                      get_sequential_wav_paths(\"data/speech_ordered/train\"), \n",
    "                                      batch_size=hp[\"batch_size\"])\n",
    "        train_dataset, val_dataset = _dataset.split(0.2)\n",
    "        del _dataset\n",
    "        base_train_dataloader = DataLoader(train_dataset, shuffle=SHUFFLE)\n",
    "        base_val_dataloader = DataLoader(val_dataset)\n",
    "\n",
    "        def train_step(engine, batch):\n",
    "            global pf_train_totals, pf_train_num_loops\n",
    "            pf_train_forward = time.perf_counter_ns()                               ###\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            y_pred = model(x)\n",
    "            pf_train_totals[0] += (time.perf_counter_ns() - pf_train_forward)       ###\n",
    "            pf_train_back = time.perf_counter_ns()                                  ###\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pf_train_totals[1] += (time.perf_counter_ns() - pf_train_back)          ###\n",
    "            pf_train_num_loops += 1                                                 ###\n",
    "            return loss.item()\n",
    "        \n",
    "        trainer = Engine(train_step)\n",
    "        register_custom_events(trainer)\n",
    "        RunningAverage(output_transform=lambda x: x).attach(trainer,'loss')\n",
    "        pbar = ProgressBar(desc=\"Training Epoch\")\n",
    "        pbar.attach(trainer,['loss'])\n",
    "\n",
    "        trainer.add_event_handler(Events.STARTED, set_engine_custom_keys)\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED(once=1),set_iteration_ceiling)\n",
    "\n",
    "        train_dataloader = MultiFrameLoader(base_train_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"],\n",
    "                                              hp[\"num_frames\"], engine=trainer, output_transform=lambda x: x.unsqueeze(1))\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        def val_step(engine, batch):\n",
    "            global pf_eval_total, pf_eval_num_loops\n",
    "            pf_eval_forward = time.perf_counter_ns()                                ###\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                pf_eval_total += (time.perf_counter_ns() - pf_eval_forward)         ###\n",
    "                pf_eval_num_loops += 1                                              ###\n",
    "                for i in range(y_pred.shape[2]):\n",
    "                    proc_frame_constructor.add_frame(y_pred[:,:,i,:])\n",
    "                    clean_frame_constructor.add_frame(y[:,:,i,:])\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio()\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio()\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    return y_pred, y, {\"stitch_proc\": y_pred_stitch, \"stitch_clean\": y_stitch}\n",
    "\n",
    "                return y_pred, y\n",
    "        \n",
    "        validator = Engine(val_step)\n",
    "        val_metrics: dict[str, Metric] = {\n",
    "            \"loss\": Loss(criterion, output_transform=lambda x: (x[0],x[1])),\n",
    "            \"pesq\": PESQMetric(),\n",
    "            \"stoi\": STOIMetric()\n",
    "        }\n",
    "        for name, metric in val_metrics.items():\n",
    "            metric.attach(validator, name)\n",
    "        RunningAverage(output_transform=lambda x: criterion(x[0],x[1]).item()).attach(validator,'running_loss')\n",
    "        pbar = ProgressBar(desc=\"Validation\")\n",
    "        pbar.attach(validator,['running_loss'])\n",
    "        \n",
    "        checkpoint_to_save = {\"model\":model}\n",
    "        checkpoint_handler = Checkpoint(\n",
    "            checkpoint_to_save, f\"saved_models/cnn_{datestring_at_start}\",\n",
    "            filename_prefix=\"best\", score_function=lambda eng: eng.state.metrics['pesq'],n_saved=2\n",
    "        )\n",
    "        metrics_out = []\n",
    "        out[\"metrics\"] = metrics_out\n",
    "        val_dataloader = MultiFrameLoader(base_val_dataloader, hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"],\n",
    "                                              hp[\"num_frames\"], engine=trainer, output_transform=lambda x: x.unsqueeze(1))\n",
    "        trainer.add_event_handler(Events.EPOCH_COMPLETED,run_eval,validator=validator,val_frame_loader=val_dataloader)\n",
    "        trainer.add_event_handler(ValidationEvents.VALIDATION_COMPLETED,log_eval_results,validator=validator,metrics_out=metrics_out)\n",
    "        validator.add_event_handler(Events.COMPLETED, checkpoint_handler)\n",
    "\n",
    "\n",
    "        time_train = time.perf_counter_ns()\n",
    "        trainer.run(train_dataloader, max_epochs=hp[\"epochs\"])\n",
    "        out[\"total_time\"] = time.perf_counter_ns() - time_train\n",
    "        out[\"fwd\"]=pf_train_totals[0] / float(pf_train_num_loops)                        ###\n",
    "        out[\"bck\"]=pf_train_totals[1] / float(pf_train_num_loops)                        ###\n",
    "        out[\"eval\"]=pf_eval_total / float(pf_eval_num_loops)                          ###\n",
    "        \n",
    "        \n",
    "    except Exception as e:\n",
    "        print(traceback.print_exc())\n",
    "        print(e) \n",
    "    finally:\n",
    "        logging.disable(logging.NOTSET)\n",
    "        if \"model\" in locals():\n",
    "            if hp[\"save\"]:\n",
    "                torch.save(model.state_dict(),f\"saved_models/cnn_{datestring_at_start}/final.pt\")\n",
    "                json_dict = {k: out[k] for k in out.keys() - {'model'}}\n",
    "                with open(f\"saved_models/cnn_{datestring_at_start}/out.json\",\"w\") as f:\n",
    "                    json.dump(json_dict,f)\n",
    "            return out\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "if CNN_RUN_ON_LOAD:\n",
    "    cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.segan import Generator\n",
    "from models.tcnn import TCNN\n",
    "from models.rhrnet import RHRNet\n",
    "from models.wavecrn import ConvBSRU\n",
    "from pesq.cypesq import NoUtterancesError\n",
    "from utils.rhr_hp_load import load_rnn_hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_test(model: Generator, dl: FrameLoader, hp: dict = gan_hp):\n",
    "    results = []\n",
    "    try:\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        z = torch.zeros((hp[\"batch_size\"],1024,8)).to(device=device)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl):\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x,z)\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    for i in range(y_stitch.shape[0]):\n",
    "                        try:\n",
    "                            sq = calc_pesq(y_stitch[i], y_pred_stitch[i])\n",
    "                            si = calc_stoi(y_stitch[i], y_pred_stitch[i])\n",
    "                            ssnr = calc_snrseg(y_stitch[i], y_pred_stitch[i])\n",
    "                            results.append({\"pesq\":sq, \"stoi\":si, \"ssnr\":ssnr})\n",
    "                        except NoUtterancesError as e:\n",
    "                            print(\"NoUtterancesError\")\n",
    "                        except Exception as e:\n",
    "                            print(\"exception\")\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def crn_test(model: ConvBSRU, dl: FrameLoader, hp: dict = crn_hp):\n",
    "    results = []\n",
    "    try:\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl):\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    for i in range(y_stitch.shape[0]):\n",
    "                        try:\n",
    "                            sq = calc_pesq(y_stitch[i], y_pred_stitch[i])\n",
    "                            si = calc_stoi(y_stitch[i], y_pred_stitch[i])\n",
    "                            ssnr = calc_snrseg(y_stitch[i], y_pred_stitch[i])\n",
    "                            results.append({\"pesq\":sq, \"stoi\":si, \"ssnr\":ssnr})\n",
    "                        except Exception as e:\n",
    "                            traceback.print_exc()\n",
    "                            raise Exception()\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "    \n",
    "def rnn_test(model: RHRNet, dl: FrameLoader, hp: dict = rnn_hp):\n",
    "    results = []\n",
    "    try:\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl):\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                proc_frame_constructor.add_frame(y_pred)\n",
    "                clean_frame_constructor.add_frame(y)\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    for i in range(y_stitch.shape[0]):\n",
    "                        try:\n",
    "                            sq = calc_pesq(y_stitch[i], y_pred_stitch[i])\n",
    "                            si = calc_stoi(y_stitch[i], y_pred_stitch[i])\n",
    "                            ssnr = calc_snrseg(y_stitch[i], y_pred_stitch[i])\n",
    "                            results.append({\"pesq\":sq, \"stoi\":si, \"ssnr\":ssnr})\n",
    "                        except NoUtterancesError as e:\n",
    "                            print(\"NoUtterancesError\")\n",
    "                            continue\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def cnn_test(model: TCNN, dl: MultiFrameLoader, hp: dict = cnn_hp):\n",
    "    results = []\n",
    "    try:\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], hp[\"batch_size\"])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(dl):\n",
    "                x, y = batch[0].to(device), batch[1].to(device)\n",
    "                y_pred = model(x)\n",
    "                for i in range(y_pred.shape[2]):\n",
    "                    # print(\"shape\" + str(y_pred.shape))\n",
    "                    # print(\"slice shape\" + str(y_pred[:,:,i,:].shape))\n",
    "                    # print(\"std slice shape\" + str(standardize_batch(y_pred[:,:,i,:]).shape))\n",
    "                    proc_frame_constructor.add_frame(y_pred[:,:,i,:])\n",
    "                    clean_frame_constructor.add_frame(y[:,:,i,:])\n",
    "                if batch[2]:    #   Frame fully constructed\n",
    "                    y_pred_stitch = proc_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    y_stitch = clean_frame_constructor.get_current_audio().numpy(force=True)\n",
    "                    proc_frame_constructor.reset()\n",
    "                    clean_frame_constructor.reset()\n",
    "                    for i in range(y_stitch.shape[0]):\n",
    "                        try:\n",
    "                            sq = calc_pesq(y_stitch[i], y_pred_stitch[i])\n",
    "                            si = calc_stoi(y_stitch[i], y_pred_stitch[i])\n",
    "                            ssnr = calc_snrseg(y_stitch[i], y_pred_stitch[i])\n",
    "                            results.append({\"pesq\":sq, \"stoi\":si, \"ssnr\":ssnr})\n",
    "                        except NoUtterancesError as e:\n",
    "                            print(\"NoUtterancesError\")\n",
    "                            continue\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "run_base_test = False\n",
    "if run_base_test:\n",
    "    snr_version = \"high_snr\"\n",
    "    #gan\n",
    "    print(\"Testing GAN...\")\n",
    "    model = Generator().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"/vol/research/FYP_Leo/speech_denoising_fyp/saved_models/final/gan_29-04-2025--11-58-59/best_checkpoint_1.8107.pt\")[\"gen\"])\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"),get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),gan_hp[\"batch_size\"])\n",
    "    dl = FrameLoader(ds, gan_hp[\"frame_size\"], gan_hp[\"frame_shift\"], gan_hp[\"batch_size\"], \n",
    "                    output_transform=lambda x: x.view((gan_hp[\"batch_size\"],1,-1)))\n",
    "    gan_results = gan_test(model, dl, gan_hp)\n",
    "    # with open(\"results/gan_test-\"+datetime_string()+\".json\",\"w\") as file:\n",
    "    #     json.dump(gan_results, file)\n",
    "    del model, ds, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #crn\n",
    "    print(\"Testing CRN...\")\n",
    "    model = ConvBSRU(frame_size=crn_hp[\"frame_size\"], conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"/vol/research/FYP_Leo/speech_denoising_fyp/saved_models/final/crn_20-04-2025--12-49-08/best_model_1.7157.pt\"))\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"),get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),crn_hp[\"batch_size\"])\n",
    "    dl = FrameLoader(ds, crn_hp[\"frame_size\"], crn_hp[\"frame_shift\"], crn_hp[\"batch_size\"], \n",
    "                    output_transform=lambda x: x.view((crn_hp[\"batch_size\"],1,-1)))\n",
    "    crn_results = crn_test(model, dl, crn_hp)\n",
    "    # with open(\"results/crn_test-\"+datetime_string()+\".json\",\"w\") as file:\n",
    "    #     json.dump(crn_results, file)\n",
    "    del model, ds, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #rnn\n",
    "    print(\"Testing RNN...\")\n",
    "    import yaml\n",
    "    from models.rhrnetdir.Arg_Parser import Recursive_Parse\n",
    "    _rnn_hp = Recursive_Parse(yaml.load(\n",
    "        open('models/rhrnetdir/rhrnet_hyperparameters.yaml', encoding='utf-8'),\n",
    "        Loader=yaml.Loader\n",
    "        )) \n",
    "    model = RHRNet(_rnn_hp).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"/vol/research/FYP_Leo/speech_denoising_fyp/saved_models/final/rnn_21-04-2025--01-00-43/best_model_2.0291.pt\"))\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"),get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),rnn_hp[\"batch_size\"])\n",
    "    dl = FrameLoader(ds, rnn_hp[\"frame_size\"], rnn_hp[\"frame_shift\"], rnn_hp[\"batch_size\"])\n",
    "    rnn_results = rnn_test(model, dl, rnn_hp)\n",
    "    # with open(\"results/rnn_test-\"+datetime_string()+\".json\",\"w\") as file:\n",
    "    #     json.dump(rnn_results, file)\n",
    "    del model, ds, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    #cnn\n",
    "    print(\"Testing CNN...\")\n",
    "    model = TCNN().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"/vol/research/FYP_Leo/speech_denoising_fyp/saved_models/final/cnn_08-05-2025--12-02-34/best_model_1.9495.pt\"))\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"),get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),cnn_hp[\"batch_size\"])\n",
    "    dl = MultiFrameLoader(ds,cnn_hp[\"frame_size\"], cnn_hp[\"frame_shift\"],cnn_hp[\"batch_size\"],cnn_hp[\"num_frames\"],output_transform=lambda x: x.unsqueeze(1))\n",
    "    cnn_results = cnn_test(model, dl, cnn_hp)\n",
    "    # with open(\"results/cnn_test-\"+datetime_string()+\".json\",\"w\") as file:\n",
    "    #     json.dump(cnn_results, file)\n",
    "    del model, ds, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    all_res = {\"gan\":gan_results, \"crn\":crn_results, \"rnn\":rnn_results, \"cnn\":cnn_results}\n",
    "    with open(f\"results/{snr_version}_{datetime_string()}.json\",\"w\") as file:\n",
    "        json.dump(all_res, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Speed Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_one_sec_test(model: Generator, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    try:\n",
    "        results = []\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        rnd = random.Random()\n",
    "        z = torch.zeros((hp[\"batch_size\"],1024,8)).to(device=device)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            perf_time = time.perf_counter_ns()\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = rnd.randint(0, sample[0].shape[-1]-hp[\"frame_size\"])\n",
    "            slc = (start, start + hp[\"frame_size\"])\n",
    "            noisy, clean = sample[0].squeeze()[slice(*slc)].clone().detach().view((1,1,-1)), sample[1].squeeze()[slice(*slc)].clone().detach().view((1,1,-1))\n",
    "            with torch.no_grad():\n",
    "                x, y = noisy.to(device=device), clean.to(device=device)\n",
    "                nn.init.normal_(z)\n",
    "                y_pred: torch.Tensor = model(x, z)\n",
    "            perf_time = time.perf_counter_ns() - perf_time\n",
    "            y_pred_np = y_pred.squeeze().numpy(force=True)\n",
    "            y_np = y.squeeze().numpy(force=True)\n",
    "            # x_np = x.numpy(force=True)\n",
    "            try:\n",
    "                sq = calc_pesq(y_np, y_pred_np)\n",
    "                si = calc_stoi(y_np, y_pred_np)\n",
    "                ssnr = calc_snrseg(y_np, y_pred_np)\n",
    "            except:\n",
    "                # traceback.print_exc()\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\": ssnr, \"time\": perf_time\n",
    "            }\n",
    "            results.append(res)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "        pass\n",
    "    finally:\n",
    "        return results\n",
    "    \n",
    "def crn_one_sec_test(model: ConvBSRU, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    '''`dl` should be a DataLoader that provides the full audio file, not a FrameLoader.'''\n",
    "    try:\n",
    "        results = []\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        rnd = random.Random()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        # noisy_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        z = torch.zeros((hp[\"batch_size\"],1024,8)).to(device=device)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = rnd.randint(0, sample[0].shape[-1]-SAMPLE_RATE)\n",
    "            slc = (start, start + SAMPLE_RATE)\n",
    "            _noisy, _clean = sample[0][:,:,slice(*slc)].clone().detach(), sample[1][:,:,slice(*slc)].clone().detach()\n",
    "            _,_,pad = calc_windowing(_noisy.shape[-1],hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            _noisy, _clean = torch.nn.functional.pad(_noisy,(0,pad), value=0.0), torch.nn.functional.pad(_clean,(0,pad), value=0.0)\n",
    "            noisy, clean = get_all_frames(_noisy,hp[\"frame_size\"], hp[\"frame_shift\"]), get_all_frames(_clean,hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            perf_time = time.perf_counter_ns()\n",
    "            for j in range(noisy.shape[1]):\n",
    "                batch = noisy.narrow(1, j, 1), clean.narrow(1, j, 1)    #   Frame num dimension becomes 1, can be reused as channel index\n",
    "                with torch.no_grad():\n",
    "                    x, y = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                    y_pred: torch.Tensor = model(x, z)\n",
    "                    proc_frame_constructor.add_frame(y_pred)\n",
    "                    clean_frame_constructor.add_frame(y)\n",
    "                    # noisy_frame_constructor.add_frame(x)\n",
    "\n",
    "            perf_time = time.perf_counter_ns() - perf_time\n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze()\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze()\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "            # x = noisy_frame_constructor.get_current_audio()\n",
    "            y_pred_np = y_pred.numpy(force=True)\n",
    "            y_np = y.numpy(force=True)\n",
    "            # x_np = x.numpy(force=True)\n",
    "            try:\n",
    "                sq = calc_pesq(y_np, y_pred_np)\n",
    "                si = calc_stoi(y_np, y_pred_np)\n",
    "                ssnr = calc_snrseg(y_np, y_pred_np)\n",
    "            except:\n",
    "                traceback.print_exc()\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time\n",
    "            }\n",
    "            results.append(res)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def rnn_one_sec_test(model: RHRNet, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    try:\n",
    "        results = []\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        rnd = random.Random()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        # noisy_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = rnd.randint(0, sample[0].shape[-1]-SAMPLE_RATE)\n",
    "            slc = (start, start + SAMPLE_RATE)\n",
    "            _noisy, _clean = sample[0].squeeze()[slice(*slc)].clone().detach(), sample[1].squeeze()[slice(*slc)].clone().detach()\n",
    "            _,_,pad = calc_windowing(_noisy.shape[-1],hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            noisy_pad, clean_pad = torch.nn.functional.pad(_noisy,(0,pad), value=0.0), torch.nn.functional.pad(_clean,(0,pad), value=0.0)\n",
    "            noisy, clean = get_all_frames(noisy_pad,hp[\"frame_size\"], hp[\"frame_shift\"]), get_all_frames(clean_pad,hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            perf_time = time.perf_counter_ns()\n",
    "            for j in range(noisy.shape[1]):\n",
    "                batch = noisy.narrow(1, j, 1).squeeze(1), clean.narrow(1, j, 1).squeeze(1)\n",
    "                with torch.no_grad():\n",
    "                    x, y = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                    y_pred: torch.Tensor = model(x)\n",
    "                    proc_frame_constructor.add_frame(y_pred)\n",
    "                    clean_frame_constructor.add_frame(y)\n",
    "                    # noisy_frame_constructor.add_frame(x)\n",
    "\n",
    "            perf_time = time.perf_counter_ns() - perf_time\n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze()\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze()\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "            # display.display(Audio(y_pred, rate=SAMPLE_RATE))\n",
    "            # display.display(Audio(y, rate=SAMPLE_RATE))\n",
    "            # x = noisy_frame_constructor.get_current_audio()\n",
    "            y_pred_np = y_pred.numpy(force=True)\n",
    "            y_np = y.numpy(force=True)\n",
    "            # x_np = x.numpy(force=True)\n",
    "            try:\n",
    "                sq = calc_pesq(y_np, y_pred_np)\n",
    "                si = calc_stoi(y_np, y_pred_np)\n",
    "                ssnr = calc_snrseg(y_np, y_pred_np)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time\n",
    "            }\n",
    "            results.append(res)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def cnn_one_sec_test(model: TCNN, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    try:\n",
    "        results = []\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        rnd = random.Random()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        # noisy_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = rnd.randint(0, sample[0].shape[-1]-SAMPLE_RATE)\n",
    "            slc = (start, start + SAMPLE_RATE)\n",
    "            _noisy, _clean = sample[0][:,:,slice(*slc)].clone().detach(), sample[1][:,:,slice(*slc)].clone().detach()\n",
    "            _,_,pad = calc_windowing(_noisy.shape[-1],hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            _noisy, _clean = torch.nn.functional.pad(_noisy,(0,pad), value=0.0), torch.nn.functional.pad(_clean,(0,pad), value=0.0)\n",
    "            noisy, clean = get_all_frames(_noisy,hp[\"frame_size\"], hp[\"frame_shift\"]), get_all_frames(_clean,hp[\"frame_size\"], hp[\"frame_shift\"])\n",
    "            j = 0\n",
    "            j_end = noisy.shape[1]\n",
    "            perf_time = time.perf_counter_ns()\n",
    "            _num_frames = min(hp[\"num_frames\"],j_end - j)\n",
    "            while j < j_end:\n",
    "                batch = noisy.narrow(1,j,_num_frames).unsqueeze(1), clean.narrow(1,j,_num_frames).unsqueeze(1)\n",
    "                j += hp[\"num_frames\"]\n",
    "                with torch.no_grad():\n",
    "                    x, y = batch[0].to(device=device), batch[1].to(device=device)\n",
    "                    y_pred: torch.Tensor = model(x)\n",
    "                    for i in range(y_pred.shape[2]):\n",
    "                        proc_frame_constructor.add_frame(y_pred[:,:,i,:])\n",
    "                        clean_frame_constructor.add_frame(y[:,:,i,:])\n",
    "                        # noisy_frame_constructor.add_frame(x[:,:,i,:])\n",
    "\n",
    "\n",
    "            perf_time = time.perf_counter_ns() - perf_time\n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze()\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze()\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "            # x = clean_frame_constructor.get_current_audio()\n",
    "            y_pred_np = y_pred.numpy(force=True)\n",
    "            y_np = y.numpy(force=True)\n",
    "            # x_np = x.numpy(force=True)\n",
    "            try:\n",
    "                sq = calc_pesq(y_np, y_pred_np)\n",
    "                si = calc_stoi(y_np, y_pred_np)\n",
    "                ssnr = calc_snrseg(y_np, y_pred_np)\n",
    "            except:\n",
    "                # traceback.print_exc()\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time\n",
    "            }\n",
    "            results.append(res)\n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally: \n",
    "        return results\n",
    "\n",
    "run_onesec_test = False\n",
    "if run_onesec_test:\n",
    "    snr_version = \"low_snr\"\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"), get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),1)\n",
    "    logging.disable(logging.DEBUG)\n",
    "\n",
    "    gan_results = []\n",
    "    print(\"Testing GAN...\")\n",
    "    dl = DataLoader(ds)\n",
    "    model = Generator().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/gan_29-04-2025--11-58-59/best_checkpoint_1.8107.pt\")[\"gen\"])\n",
    "    gan_test_hp = {\"frame_size\":16384, \"frame_shift\": 16384 - (16384//4), \"batch_size\":1}\n",
    "    gan_results = gan_one_sec_test(model, dl, gan_test_hp)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    crn_results = []\n",
    "    print(\"Testing CRN...\")\n",
    "    dl = DataLoader(ds)\n",
    "    model = ConvBSRU(frame_size=crn_hp[\"frame_size\"], conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/crn_20-04-2025--12-49-08/best_model_1.7157.pt\"))\n",
    "    crn_test_hp = {\"frame_size\":96, \"frame_shift\": 96 - (96//4), \"batch_size\":1}\n",
    "    crn_results = crn_one_sec_test(model, dl, crn_test_hp)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    rnn_results = []\n",
    "    print(\"Testing RNN...\")\n",
    "    dl = DataLoader(ds)\n",
    "    _rnn_hp = load_rnn_hp()\n",
    "    model = RHRNet(_rnn_hp).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/rnn_21-04-2025--01-00-43/best_model_2.0291.pt\"))\n",
    "    rnn_test_hp = {\"frame_size\":320, \"frame_shift\": 320 - (320//4), \"batch_size\":1}\n",
    "    rnn_results = rnn_one_sec_test(model, dl, rnn_test_hp)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cnn_results = []\n",
    "    print(\"Testing CNN...\")\n",
    "    dl = DataLoader(ds)\n",
    "    model = TCNN().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/cnn_08-05-2025--12-02-34/best_model_1.9495.pt\"))\n",
    "    cnn_test_hp = {\"frame_size\":320, \"frame_shift\": 320 - (320//4),\"num_frames\": 300, \"batch_size\":1}\n",
    "    cnn_results = cnn_one_sec_test(model, dl, cnn_test_hp)\n",
    "    print(cnn_results)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    all_res = {\"gan\":gan_results, \"crn\":crn_results, \"rnn\":rnn_results, \"cnn\":cnn_results}\n",
    "    with open(f\"results/{snr_version}_one-sec_{datetime_string()}.json\",\"w\") as file:\n",
    "        json.dump(all_res, file)\n",
    "\n",
    "### One test will be how fast can it do 1 second of audio\n",
    "### One test will be at max 10ms shift\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_10ms_test(model: Generator, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    frame_shift = 160\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        z = torch.zeros((1,1024,8)).to(device=device)\n",
    "        i = 0\n",
    "        sample: list[torch.Tensor]\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            end = frame_shift\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            j=0\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if start < 0:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                elif end > sample[0].shape[-1]:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    at_end = True\n",
    "                else:\n",
    "                    x = sample[0].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                    y = sample[1].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred: torch.Tensor = model(x, z)\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "                proc_frame_constructor.add_presliced(y_pred[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                start += frame_shift\n",
    "                end += frame_shift\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": sample[0].shape[-1], \"frames\":j\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def crn_10ms_test(model: ConvBSRU, dl: FrameLoader, hp: dict, n_samples = None):\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = 99999999999999999\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], hp[\"frame_shift\"], 1)\n",
    "        i = 1\n",
    "        perf_time = 0\n",
    "        j=0\n",
    "        for batch in tqdm(dl):\n",
    "            j+=1\n",
    "            x, y = batch[0].to(device), batch[1].to(device)\n",
    "            _perf_time = time.perf_counter_ns()\n",
    "            y_pred = model(x)\n",
    "            perf_time += time.perf_counter_ns() - _perf_time\n",
    "            proc_frame_constructor.add_frame(y_pred.clone().detach())\n",
    "            clean_frame_constructor.add_frame(y.clone().detach())\n",
    "            if batch[2]:\n",
    "                y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "                y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "                proc_frame_constructor.reset()\n",
    "                clean_frame_constructor.reset()\n",
    "\n",
    "                try:\n",
    "                    sq = calc_pesq(y, y_pred)\n",
    "                    si = calc_stoi(y, y_pred)\n",
    "                    ssnr = calc_snrseg(y, y_pred)\n",
    "                except NoUtterancesError as e:\n",
    "                    continue\n",
    "                res = {\n",
    "                    \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": y_pred.shape[-1], \"frames\":j\n",
    "                }\n",
    "                results.append(res)\n",
    "\n",
    "                if i >= n_samples:\n",
    "                    break\n",
    "                i+=1\n",
    "                j=0\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def rnn_10ms_test(model: RHRNet, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    frame_shift = 160\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            end = frame_shift\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            j=0\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if start < 0:\n",
    "                    __x = sample[0][:,0:end]\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,-1).to(device=device)\n",
    "                elif end > sample[0].shape[-1]:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,-1).to(device=device)\n",
    "                    at_end = True\n",
    "                else:\n",
    "                    x = sample[0].squeeze()[start:end].clone().detach().view(1,-1).to(device=device)\n",
    "                    y = sample[1].squeeze()[start:end].clone().detach().view(1,-1).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred: torch.Tensor = model(x)\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "                proc_frame_constructor.add_presliced(y_pred[:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                start += frame_shift\n",
    "                end += frame_shift\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": y_pred.shape[-1]\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def cnn_10ms_test(model: TCNN, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    from collections import deque\n",
    "    frame_shift = 160\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            x_pad = torch.nn.functional.pad(sample[0].squeeze(),(abs(start),0),value=0.0)\n",
    "            y_pad = torch.nn.functional.pad(sample[1].squeeze(),(abs(start),0),value=0.0)\n",
    "            _,_,pad = calc_windowing(x_pad.shape[0],hp[\"frame_size\"],frame_shift)\n",
    "            x_pad = torch.nn.functional.pad(x_pad,(0,pad),value=0.0)\n",
    "            y_pad = torch.nn.functional.pad(y_pad,(0,pad),value=0.0)\n",
    "            x_frames = get_all_frames(x_pad,hp[\"frame_size\"],frame_shift)\n",
    "            y_frames = get_all_frames(y_pad,hp[\"frame_size\"],frame_shift)\n",
    "            \n",
    "            first_window=0\n",
    "            windows_count=0\n",
    "            j=0\n",
    "            y_pred: torch.Tensor\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if first_window+windows_count >= x_frames.shape[1]-1:\n",
    "                    at_end = True\n",
    "                # print(j, flush=True)\n",
    "                windows_count+=1\n",
    "                if windows_count >= hp[\"num_frames\"]:\n",
    "                    windows_count = hp[\"num_frames\"]\n",
    "                    first_window += 1\n",
    "                x = x_frames.narrow(1,first_window, windows_count)\n",
    "                x = x.view(1,1,x.shape[1], hp[\"frame_size\"]).to(device=device)\n",
    "                y = y_frames.narrow(1,first_window, windows_count)\n",
    "                y = y.view(1,1,y.shape[1], hp[\"frame_size\"]).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred = model(x)\n",
    "                proc_frame_constructor.add_presliced(y_pred[0,0,-1,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[0,0,-1,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": y_pred.shape[-1], \"frames\":j\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "run_10ms_tests = True\n",
    "if run_10ms_tests:\n",
    "    snr_version = \"high_snr\"\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"), get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),1)\n",
    "    logging.disable(logging.DEBUG)\n",
    "\n",
    "    gan_results = []\n",
    "    print(\"Testing GAN...\")\n",
    "    gan_test_hp = {\"frame_size\":16384, \"frame_shift\": 160, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    model = Generator().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/gan_29-04-2025--11-58-59/best_checkpoint_1.8107.pt\")[\"gen\"])\n",
    "    gan_results = gan_10ms_test(model, dl, gan_test_hp,1000)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    crn_results = []\n",
    "    print(\"Testing CRN...\")\n",
    "    crn_test_hp = {\"frame_size\":96, \"frame_shift\": 48, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    fl = FrameLoader(dl, crn_test_hp[\"frame_size\"],crn_test_hp[\"frame_shift\"],1, output_transform=lambda x: x.view(1,1,-1))\n",
    "    model = ConvBSRU(frame_size=crn_test_hp[\"frame_size\"], conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/crn_20-04-2025--12-49-08/best_model_1.7157.pt\"))\n",
    "    crn_results = crn_10ms_test(model, fl, crn_test_hp,1000)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    rnn_results = []\n",
    "    print(\"Testing RNN...\")\n",
    "    rnn_test_hp = {\"frame_size\":320, \"frame_shift\": 160, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    _rnn_hp = load_rnn_hp()\n",
    "    model = RHRNet(_rnn_hp).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/rnn_21-04-2025--01-00-43/best_model_2.0291.pt\"))\n",
    "    rnn_results = rnn_10ms_test(model, dl, rnn_test_hp,1000)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cnn_results = []\n",
    "    print(\"Testing CNN...\")\n",
    "    cnn_test_hp = {\"frame_size\":320, \"frame_shift\": 160,\"num_frames\": 200, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    model = TCNN().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/cnn_08-05-2025--12-02-34/best_model_1.9495.pt\"))\n",
    "    cnn_results = cnn_10ms_test(model, dl, cnn_test_hp,1000)\n",
    "    print(cnn_results)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    all_res = {\"gan\":gan_results, \"crn\":crn_results, \"rnn\":rnn_results, \"cnn\":cnn_results}\n",
    "    with open(f\"results/{snr_version}_10ms_{datetime_string()}.json\",\"w\") as file:\n",
    "        json.dump(all_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res = {\"gan\":gan_results, \"crn\":crn_results, \"rnn\":rnn_results, \"cnn\":cnn_results}\n",
    "with open(f\"results/{snr_version}_10ms_{datetime_string()}.json\",\"w\") as file:\n",
    "    json.dump(all_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_3ms_test(model: Generator, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    frame_shift = 48\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        z = torch.zeros((1,1024,8)).to(device=device)\n",
    "        i = 0\n",
    "        sample: list[torch.Tensor]\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            end = frame_shift\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            j=0\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if start < 0:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                elif end > sample[0].shape[-1]:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    at_end = True\n",
    "                else:\n",
    "                    x = sample[0].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                    y = sample[1].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred: torch.Tensor = model(x, z)\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "                proc_frame_constructor.add_presliced(y_pred[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                start += frame_shift\n",
    "                end += frame_shift\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": sample[0].shape[-1], \"frames\":j\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def crn_3ms_test(model: ConvBSRU, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    frame_shift = 48\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        i = 0\n",
    "        sample: list[torch.Tensor]\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            end = frame_shift\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            j=0\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if start < 0:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,1,-1).to(device=device)\n",
    "                elif end > sample[0].shape[-1]:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,1,-1).to(device=device)\n",
    "                    at_end = True\n",
    "                else:\n",
    "                    x = sample[0].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                    y = sample[1].squeeze()[start:end].clone().detach().view(1,1,-1).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred: torch.Tensor = model(x)\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "                proc_frame_constructor.add_presliced(y_pred[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[:,:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                start += frame_shift\n",
    "                end += frame_shift\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": sample[0].shape[-1], \"frames\":j\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def rnn_3ms_test(model: RHRNet, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    frame_shift = 48\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            end = frame_shift\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            j=0\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if start < 0:\n",
    "                    __x = sample[0][:,0:end]\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[0:end].clone().detach(),(abs(start),0),value=0.0).view(1,-1).to(device=device)\n",
    "                elif end > sample[0].shape[-1]:\n",
    "                    x = torch.nn.functional.pad(sample[0].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,-1).to(device=device)\n",
    "                    y = torch.nn.functional.pad(sample[1].squeeze()[start:sample[0].shape[-1]].clone().detach(),(0,end-sample[0].shape[-1]),value=0.0).view(1,-1).to(device=device)\n",
    "                    at_end = True\n",
    "                else:\n",
    "                    x = sample[0].squeeze()[start:end].clone().detach().view(1,-1).to(device=device)\n",
    "                    y = sample[1].squeeze()[start:end].clone().detach().view(1,-1).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred: torch.Tensor = model(x)\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "                proc_frame_constructor.add_presliced(y_pred[:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[:,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                start += frame_shift\n",
    "                end += frame_shift\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": y_pred.shape[-1]\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "def cnn_3ms_test(model: TCNN, dl: DataLoader, hp: dict, n_samples = None):\n",
    "    from collections import deque\n",
    "    frame_shift = 48\n",
    "    results = []\n",
    "    try:\n",
    "        if n_samples == None:\n",
    "            n_samples = len(dl)\n",
    "        model.eval()\n",
    "        sample: torch.Tensor\n",
    "        proc_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        clean_frame_constructor = FrameReconstructor(hp[\"frame_size\"], frame_shift, 1)\n",
    "        i = 0\n",
    "        for sample in tqdm(dl):\n",
    "            if i >= n_samples:\n",
    "                break\n",
    "            i+=1\n",
    "            start = frame_shift - hp[\"frame_size\"]\n",
    "            x: torch.Tensor\n",
    "            y: torch.Tensor\n",
    "            at_end = False\n",
    "            perf_time = 0\n",
    "            x_pad = torch.nn.functional.pad(sample[0].squeeze(),(abs(start),0),value=0.0)\n",
    "            y_pad = torch.nn.functional.pad(sample[1].squeeze(),(abs(start),0),value=0.0)\n",
    "            _,_,pad = calc_windowing(x_pad.shape[0],hp[\"frame_size\"],frame_shift)\n",
    "            x_pad = torch.nn.functional.pad(x_pad,(0,pad),value=0.0)\n",
    "            y_pad = torch.nn.functional.pad(y_pad,(0,pad),value=0.0)\n",
    "            x_frames = get_all_frames(x_pad,hp[\"frame_size\"],frame_shift)\n",
    "            y_frames = get_all_frames(y_pad,hp[\"frame_size\"],frame_shift)\n",
    "            \n",
    "            first_window=0\n",
    "            windows_count=0\n",
    "            j=0\n",
    "            y_pred: torch.Tensor\n",
    "            while not at_end:\n",
    "                j+=1\n",
    "                if first_window+windows_count >= x_frames.shape[1]-1:\n",
    "                    at_end = True\n",
    "                # print(j, flush=True)\n",
    "                windows_count+=1\n",
    "                if windows_count >= hp[\"num_frames\"]:\n",
    "                    windows_count = hp[\"num_frames\"]\n",
    "                    first_window += 1\n",
    "                x = x_frames.narrow(1,first_window, windows_count)\n",
    "                x = x.view(1,1,x.shape[1], hp[\"frame_size\"]).to(device=device)\n",
    "                y = y_frames.narrow(1,first_window, windows_count)\n",
    "                y = y.view(1,1,y.shape[1], hp[\"frame_size\"]).to(device=device)\n",
    "                \n",
    "                _perf_time = time.perf_counter_ns()\n",
    "                y_pred = model(x)\n",
    "                proc_frame_constructor.add_presliced(y_pred[0,0,-1,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                clean_frame_constructor.add_presliced(y[0,0,-1,hp[\"frame_size\"]-frame_shift:hp[\"frame_size\"]].clone().detach())\n",
    "                perf_time += (time.perf_counter_ns() - _perf_time)\n",
    "            \n",
    "            y_pred = proc_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            y = clean_frame_constructor.get_current_audio().squeeze().numpy(force=True)\n",
    "            proc_frame_constructor.reset()\n",
    "            clean_frame_constructor.reset()\n",
    "\n",
    "            try:\n",
    "                sq = calc_pesq(y, y_pred)\n",
    "                si = calc_stoi(y, y_pred)\n",
    "                ssnr = calc_snrseg(y, y_pred)\n",
    "            except NoUtterancesError as e:\n",
    "                continue\n",
    "            res = {\n",
    "                \"pesq\": sq, \"stoi\": si, \"ssnr\":ssnr, \"time\": perf_time, \"audio_sample_len\": y_pred.shape[-1], \"frames\":j\n",
    "            }\n",
    "            results.append(res)\n",
    "                \n",
    "    except:\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        return results\n",
    "\n",
    "run_3ms_tests = True\n",
    "if run_3ms_tests:\n",
    "    snr_version = \"high_snr\"\n",
    "    ds = SortedBatchDataset(get_sequential_wav_paths(f\"data/mixed/test/{snr_version}\"), get_sequential_wav_paths(f\"data/speech_ordered/test/{snr_version}\"),1)\n",
    "    logging.disable(logging.DEBUG)\n",
    "\n",
    "    gan_results = []\n",
    "    print(\"Testing GAN...\")\n",
    "    gan_test_hp = {\"frame_size\":16384, \"frame_shift\": 48, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    model = Generator().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/gan_29-04-2025--11-58-59/best_checkpoint_1.8107.pt\")[\"gen\"])\n",
    "    gan_results = gan_3ms_test(model, dl, gan_test_hp,300)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    crn_results = []\n",
    "    print(\"Testing CRN...\")\n",
    "    crn_test_hp = {\"frame_size\":96, \"frame_shift\": 48, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    model = ConvBSRU(frame_size=crn_test_hp[\"frame_size\"], conv_channels=256, stride=48, num_layers=6, dropout=0.0).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/crn_20-04-2025--12-49-08/best_model_1.7157.pt\"))\n",
    "    crn_results = crn_3ms_test(model, dl, crn_test_hp,300)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    rnn_results = []\n",
    "    print(\"Testing RNN...\")\n",
    "    rnn_test_hp = {\"frame_size\":320, \"frame_shift\": 48, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    _rnn_hp = load_rnn_hp()\n",
    "    model = RHRNet(_rnn_hp).to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/rnn_21-04-2025--01-00-43/best_model_2.0291.pt\"))\n",
    "    rnn_results = rnn_3ms_test(model, dl, rnn_test_hp,300)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    cnn_results = []\n",
    "    print(\"Testing CNN...\")\n",
    "    cnn_test_hp = {\"frame_size\":320, \"frame_shift\": 48,\"num_frames\": 300, \"batch_size\":1}\n",
    "    dl = DataLoader(ds)\n",
    "    model = TCNN().to(device=device)\n",
    "    model.load_state_dict(torch.load(r\"saved_models/final/cnn_08-05-2025--12-02-34/best_model_1.9495.pt\"))\n",
    "    cnn_results = cnn_3ms_test(model, dl, cnn_test_hp,300)\n",
    "    print(cnn_results)\n",
    "    del model, dl\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "    all_res = {\"gan\":gan_results, \"crn\":crn_results, \"rnn\":rnn_results, \"cnn\":cnn_results}\n",
    "    with open(f\"results/{snr_version}_3ms_{datetime_string()}.json\",\"w\") as file:\n",
    "        json.dump(all_res, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gan\n",
      "Audio Duration - 12532 seconds\n",
      "Time Spent processing - 761.0 seconds\n",
      "Average frame process time - \\num{0.6069721550853251 \\pm 0.0087605113700968}\n",
      "PESQ - \\num{1.580843438744545 \\pm 0.20209189055903545}\n",
      "STOI - \\num{0.909417623193533 \\pm 0.040520907140753945}\n",
      "SSNR - \\num{6.55778811408832 \\pm 1.5346181403630277}\n",
      "crn\n",
      "Audio Duration - 12533 seconds\n",
      "Time Spent processing - 1173500.0 seconds\n",
      "Average frame process time - \\num{360.8187034415579 \\pm 234.49055370293144}\n",
      "PESQ - \\num{1.8796547166109085 \\pm 0.2874725416674613}\n",
      "STOI - \\num{0.9247800024814373 \\pm 0.038813890293902205}\n",
      "SSNR - \\num{7.178437906680332 \\pm 1.8109510368759507}\n",
      "rnn\n",
      "Audio Duration - 12540 seconds\n",
      "Time Spent processing - 2135.0 seconds\n",
      "Average frame process time - \\num{1.7036576129300804 \\pm 0.01170476001600824}\n",
      "PESQ - \\num{2.116352467536926 \\pm 0.2904823246083804}\n",
      "STOI - \\num{0.9359856942485169 \\pm 0.036673957559702396}\n",
      "SSNR - \\num{8.364823564455847 \\pm 1.8781794491533674}\n",
      "cnn\n",
      "Audio Duration - 12530 seconds\n",
      "Time Spent processing - 3644.0 seconds\n",
      "Average frame process time - \\num{2.900630241680933 \\pm 0.03111678536879826}\n",
      "PESQ - \\num{1.6617905411720275 \\pm 0.24985185073043908}\n",
      "STOI - \\num{0.9120377438593484 \\pm 0.03928414623842483}\n",
      "SSNR - \\num{6.89919889674026 \\pm 1.4381873459595294}\n",
      "dict_keys(['pesq', 'stoi', 'ssnr', 'time', 'audio_sample_len', 'frames'])\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "with open(\"results/high_snr_10ms_13-05-2025--20-56-08.json\",\"r\") as file:\n",
    "    results = json.load(file)\n",
    "\n",
    "for i, r in enumerate(results[\"rnn\"]):\n",
    "    r[\"frames\"] = results[\"cnn\"][i][\"frames\"]\n",
    "    \n",
    "\n",
    "for mdl, stats in results.items():\n",
    "    # pesqs = [x[\"pesq\"] for x in stats]\n",
    "    # stois = [x[\"stoi\"] for x in stats]\n",
    "    # ssnrs = [x[\"ssnr\"] for x in stats]\n",
    "    # print(f\"PESQ - Mean: {np.mean(pesqs)} | Std: {np.std(pesqs)}\")\n",
    "    # print(f\"STOI - Mean: {np.mean(stois)} | Std: {np.std(stois)}\")\n",
    "    # print(f\"SSNR - Mean: {np.mean(ssnrs)} | Std: {np.std(ssnrs)}\")\n",
    "\n",
    "    print(mdl)\n",
    "    pesqs = [x[\"pesq\"] for x in stats]\n",
    "    stois = [x[\"stoi\"] for x in stats]\n",
    "    ssnrs = [x[\"ssnr\"] for x in stats]\n",
    "    perf_time = [x[\"time\"] for x in stats]\n",
    "    audio_sample_len = [x[\"audio_sample_len\"] for x in stats]\n",
    "    frames = [x[\"frames\"] for x in stats]\n",
    "    proc_time = []\n",
    "    for time, frame_count in zip(perf_time, frames):\n",
    "        proc_time.append((time / frame_count))\n",
    "\n",
    "\n",
    "    print(f\"Audio Duration - {np.sum(audio_sample_len)//SAMPLE_RATE} seconds\")\n",
    "    print(f\"Time Spent processing - {np.sum(perf_time)// (MS_TO_NS * 1000)} seconds\")\n",
    "    print(f\"Average frame process time - \\\\num{{{np.mean(proc_time)/MS_TO_NS} \\\\pm {np.std(proc_time)/MS_TO_NS}}}\")\n",
    "    print(f\"PESQ - \\\\num{{{np.mean(pesqs)} \\\\pm {np.std(pesqs)}}}\")\n",
    "    print(f\"STOI - \\\\num{{{np.mean(stois)} \\\\pm {np.std(stois)}}}\")\n",
    "    print(f\"SSNR - \\\\num{{{np.mean(ssnrs)} \\\\pm {np.std(ssnrs)}}}\")\n",
    "\n",
    "print(results[\"gan\"][0].keys())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
